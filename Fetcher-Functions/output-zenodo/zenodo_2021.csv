title,authors,abstract,year,keywords,doi,url,type
Cloud Computing Services in E-Business: Case Study in Greek Companies during Economic Recession,"Constantinos Halkiopoulos, Konstantinos Giotopoulos, Hera Antonopoulou, Panagiotis Togias, Konstantinos Theologos","<p>The Cloud Computing is a technology that will totally dominate in the future. Companies such as Google, Microsoft, Amazon and others have invested heavily in new technological structure while increasingly focus their services to take advantage of the revolutionary Cloud Computing capabilities. There are many ways of how cloud computing is changing the business and hence the digital economy. Consumers also have preceded the business to use services based on the logic of the cloud, which allows better cooperation with blogs, wikis and social networks (Facebook, Twitter). Flexibility is the ability of an enterprise to change in order to react better and faster to the needs in order to provide updated and secure services to its consumers. The purpose of this paper is to study the impact of cloud computing services in E-Business during the economic recession of Greek market. Through this research there is an effort to make an approach in order to determine if it is cloud computing widely used in enterprises and how many of the companies in the field of IT and non have adopted it. The tool for recording and evaluating of the impact between the Cloud Computing Services and E-Business was a closed-ended questionnaire. In particular, companies were asked to reply to a targeted<br>
questionnaire, with questions like, whether cloud computing is functional, easy to use, and economical of course whether they find it a safe tool for storing their business data. The research was conducted in 2016 during economic recession and the sample involved was Greek companies. The information was collected by the use of social media. To extract useful results we have made statistical analysis of collected data, which held at the middle of Economical year of 2016. We have also implemented appropriate Machine Learning techniques through the use of classification algorithms, clustering and correlation analysis in order to produce inference<br>
rules. For the implementation of the Machine Learning techniques we have used the open source software Machine Learning and Data Mining Code R. Finally we came to the fact that businesses are adopting cloud computing services as backbone of their business strategy they have great potential in this way to sustainable development in the cloud era.</p>",2021,"Cloud Computing Services, E-Business, Social Networks, R, Economic Crisis",10.5281/zenodo.5199512,,publication
Understanding the Factors Affecting the Intention to Adopt Cloud Technology among University Student of Nepal using Technology Acceptance Model,"Mala Deep Upadhaya, Nilima Dahal, Sachin Byanju, Pranit Dahal, Manish Pokharel","<p><em>One of the most crucial areas in developing technology-driven human resources is academia. Today, the use of cloud technology is burgeoning worldwide which will soon lead to the fortification of its application in academia as well. To understand the student&rsquo;s perception regarding the adoption of the cloud and the services available through it, a survey-based methodology along with Technology Acceptance Model was taken into account as a research model. n=90 responses were taken into consideration based on 5 points Likert scale and select the best answer among the total number of respondents. Based on statistical hypothesis and feature selection and it was found that among university students who study cloud as an academic course, students were more likely to adopt cloud technology with a P-value of 0.00019. It also shows that cloud-based learning can contribute to the acceptance of cloud technology if cloud education is provided early in the undergraduate program (P-value of 0.0017). On top of that, security and privacy (P-value of 0.00019) are also the factors for choosing the cloud. However, factors such as peer pressure and regulatory policies were excluded from consideration which might also influence cloud technology acceptance.</em></p>",2021,"Technology acceptance model, cloud technology, understanding level on cloud, student cloud adoption",10.5281/zenodo.4742058,,publication
Towards a Practical Solution for Data Grounding in a Semantic Web Services Environment,"Rodríguez, Miguel, Rodríguez, Jose María Alvarez, Muñoz, Diego, Paredes, Luis, Gayo, Jose Emilio Labra, Ordóñez de Pablos, Patricia","Grounding is the process in charge of linking requests and responses of web services with the semantic web services execution platform, and it is the key activity to automate their execution in a real business environment. In this paper, the authors introduce a practical solution for data grounding. On the one hand, we need a mapping language to relate data structures from services definition in WSDL documents to concepts, properties and instances of a business domain. On the other hand, two functions that perform the lowering and lifting processes using these mapping specifications are also presented.",2021,"service-oriented architectures, web services, cloud computing, semantic web, ontologies, data grounding, semantic web services, interoperability, mapping languages",10.3217/jucs-018-11-1576,,publication
TASK SCHEDULING IN CLOUD ENVIRONMENT - A SURVEY,"D. Thangamani, Dr. K. Kungumaraj","<p>Cloud computing is an emerging technology to enhance user experiences in digitalized world. It delivers on demand computing services such as platform, storage, networking services, infrastructure, software, analysis and intelligence over the internet. Cloud computing provides flexibility, efficiency, speed, security, mobility, collaboration and disaster recovery to huge data of different organization. These benefits are achieved by using virtualization techniques and task scheduling. Task scheduling maps the tasks to virtual machines based on their clients needs and improve the performance metrics like CPU and ram utilization, reducing turnaround tine and task waiting time. This paper surveyed various task scheduling algorithms to improve user experiences in cloud computing.</p>",2021,"Cloud Computing, Task Scheduling.",10.5281/zenodo.5728024,,publication
Factors influencing cloud computing adoption in higher  education institution,"Wan Abdul Rahim Wan Mohd Isa, Ahmad Iqbal Hakim Suhaimi, Nurulhuda Noordin, Afdallyna Fathiyah Harun, Juhaida Ismail, Rosshidayu Awang Teh","<p>There are few studies on factors influencing cloud computing adoption in higher education institutions. However, there are lacks of understanding of the cloud computing adoption issues in the university. The main objective of this study is to investigate factors influencing cloud computing adoption in a higher education institution. The research method involved using qualitative interviewing with relevant stakeholders and case study at one public university in Malaysia. The analysis was done by using Atlat.ti. There are eighteen factors that have been coded into three main categories of Technological, Organizational and Environmental. These are among factors to influence the decision of cloud computing adoption for a public university. The first category (Technological) consists of nine factors; (i) relative advantage, (ii) cost reduction, (iii) ease of use, (iv) compatibility, (v) operational requirement, (vi) security, (vii) sustainability, (viii) trialability and (ix) complexity, The second category (organizational) consists of four factors; (i) infrastructure readiness, (ii) top management, (iii) knowledge and IT skillset and (iv) financial. The third category (environmental) consists of five factors; (i) Cloud Service Provider, (ii) Geographical, (iii) Data Privacy, (iv) Guideline and Policy, (v) Service Level Agreement (SLA). The result may provide a reference for the adoption of cloud computing in the area of mobile learning or mobile computing. Future work involves conducting similar studies at other case studies including public and private universities in Malaysia.</p>",2021,"Cloud, Cloud computing, Cloud computing adoption, Higher education institution, Malaysia",10.11591/ijeecs.v17.i1.pp412-419,,publication
Cloud computing acceptance among public sector employees,"Mohd Talmizie Amron, Roslina Ibrahim, Nur Azaliah Abu Bakar","<p>Cloud computing is one of the platforms that drive organisations and users to be better prepared for a simpler computing platform and offers significant benefits to the quality of work. The transition from conventional computing to the virtual world helps organisations to maximise their potential. However, not all users can accept cloud computing adoption. Failure to understand the<br>
factors of user&#39;s acceptance will negatively impact the organisation&#39;s mission of empowering the technology. Therefore, this study proposes to assess to what extent the users are accepting cloud computing. This study adopts the unified theory of acceptance and use of technology (UTAUT) and six technological and human factors assessed for the Malaysian public sectors. Survey data from several ministries were analysed using partial least squares structural equation modelling (PLS-SEM). The study found out that<br>
performance expectancy, compatibility, security, mobility, information technology (IT) knowledge, and social influence had a significant impact on the user&#39;s intention to accept cloud computing. The results of this study contribute to a clear understanding of the factors affecting the Malaysian public sectors about cloud computing.<br>
&nbsp;</p>",2021,"Acceptance, Behavioural intention, Cloud computing, PLS-SEM, Public sector, UTAUT",10.12928/TELKOMNIKA.v19i1.17883,,publication
Open ecosystems help science storm the cloud,"Gentemann, Holdgraf, Abernathey, Crichton, Colliander, Kearns, Panda, Signell","<p>The core tools of science (data, software, and computers) are undergoing a rapid and historic evolution, changing what questions scientists ask and how they find answers. Earth science data are being transformed into new formats optimized for cloud storage that enable rapid analysis of multi-petabyte data sets. Data sets are moving from archive centers to vast cloud data storage, adjacent to massive server farms. Open source cloud-based data science platforms, accessed through a web-browser window, are enabling advanced, collaborative, interdisciplinary science to be performed wherever scientists can connect to the internet. Specialized software and hardware for machine learning and artificial intelligence are being integrated into data science platforms, making them more accessible to average scientists. Increasing amounts of data and computational power in the cloud are unlocking new approaches for data-driven discovery. For the first time, it is truly feasible for scientists to bring their analysis to data in the cloud without specialized cloud computing knowledge. This shift in paradigm has the potential to lower the threshold for entry, expand the science community, and increase opportunities for collaboration while promoting scientific innovation, transparency, and reproducibility. Yet, we have all witnessed promising new tools which seem harmless and beneficial at the outset become damaging or limiting. What do we need to consider as this new way of doing science is evolving?</p>",2021,"cloud computing, open science",10.5281/zenodo.4913226,,presentation
IMPACT OF CLOUD COMPUTING TECHNOLOGY FOR LIBRARY ACTIVITIES AND SERVICES,Dr. M. Sakthi Suganth,"<p>Today we are living in the age of information. Information technology plays very vital role in library science i.e. for collection, Storage, organization, processing, and analysis of information. Library filed facing many challenges in the profession due to applications of information technology. New concepts are being added to ease the practices in the libraries is also accepting many new technologies in the profession as they suit the present information handling and they satisfy needs of the knowledge society. With the advent of Information technology, libraries have become automated which is the basic need towards advancement followed by networks and more effort are towards virtual libraries. The emergence of e-publications, digital libraries, internet usage, web tools applications for libraries, consortium practices leads to the further developments in library profession. The latest technology trend in library science is use of cloud computing for various purposes and for achieving economy in library functions. Since cloud computing is a new and core area the professionals should be aware of it and also the application of cloud computing in library science.</p>",2021,,10.5281/zenodo.4570575,,publication
"Archive for article ""Radiation Exposure Determination in a Secure, Cloud-based Online Environment""","Ben C Shirley, Eliseos J Mucaki, Joan HM Knoll, Peter K Rogan","<p>This is the hardware and software archive for the article &quot;Radiation Exposure Determination in a Secure, Cloud-based Online Environment&quot; (<a href=""https://doi.org/10.1101/2021.12.09.471993"">https://doi.org/10.1101/2021.12.09.471993</a>). This archive contains a hardware description (below) and a compressed file containing a&nbsp;website (HTML, PHP, Javascript, CSS) that&nbsp;allows a user to sign in to an existing AWS Cognito account (including temporary password update, password reset), and upload samples (a directory containing a collection of image files) to AWS S3. Images are uploaded in batches, reducing the time required to upload a sample. AWS IAM policies must be preconfigured and applied to each Cognito user in order to allow users who have signed in to access their user-specific prefix in S3.</p>

<p><strong>Software</strong></p>

<p>The software contained in this archive was utilized in the associated manuscript to provide a means for users to upload samples to S3 and download reports generated by ADCI. It has been designed to work in conjunction with AWS AppStream 2.0, which mounts the user-specific directory on S3 to a streaming instance, allowing a user to access their user-specific folder on S3 while streaming ADCI.&nbsp;</p>

<p>Dependencies (not included in archive):</p>

<ul>
	<li>amazon-cognito-identity-js -&nbsp;https://github.com/aws-amplify/amplify-js/tree/main/packages/amazon-cognito-identity-js</li>
	<li>jszip -&nbsp;https://github.com/Stuk/jszip</li>
	<li>FileSaver -&nbsp;https://github.com/eligrey/FileSaver.js/</li>
</ul>

<p>&nbsp;</p>

<p>Several sections of code must be customized for&nbsp;your specific implementation. Each of these sections is marked with a comment reading &quot;[customize]&quot;:</p>

<ul>
	<li>config.js -&nbsp; Requires a preconfigured S3 bucket, Cognito User Pool,&nbsp;and Cognito Identity Pool. Enter the bucket name, user pool id, client id, region, and identity pool id.&nbsp;</li>
	<li>index.php - apply the three dependencies listed above</li>
	<li>main.js - Specify the paths on&nbsp;S3 where samples will be uploaded and reports will be downloaded. Users are differentiated by the SHA256 hash of their Cognito username (email address).</li>
	<li>example-IAM-policy.txt - This is an example of an IAM policy which must be attached to a Cognito user in order to provide access to S3. Replace &#39;[customize]&#39; with the name of your S3 bucket if it appears at the beginning on a path, or the SHA256 hash of a user&#39;s e-mail address if it appears within a path.</li>
</ul>

<p><strong>Hardware</strong>&nbsp;</p>

<p>For each new ADCI_Online streaming session, a software instance is cloned from a single base image, or &ldquo;snapshot&rdquo;, built using the AWS Image Builder tool. The snapshot is comprised of a MS-Windows&reg; Server 2016 system with ADCI preinstalled. Snapshots can be used to clone streaming instances with distinct hardware configurations belonging to either the &ldquo;General Purpose&rdquo;, &ldquo;Compute Optimized&rdquo;, and &ldquo;Graphics G4dn&rdquo; instance families. &nbsp;Table 1 includes the&nbsp;&ldquo;stream.standard.medium&rdquo; (standard), &ldquo;stream.standard.large&rdquo;, &ldquo;stream.compute.large&rdquo;, and the Graphics Processing Unit (GPU)-enabled &ldquo;stream.graphics.g4dn.xlarge&rdquo; (G4) AppStream 2.0 hardware configurations. The same snapshot may be used to clone streaming instances from the &ldquo;General Purpose&rdquo; and &ldquo;Compute Optimized&rdquo; families, however it was necessary to create a second snapshot configured as described, but instead built on G4 hardware to allow &ldquo;Graphics G4dn&rdquo; instances to be cloned. By default, a streaming session boots using the standard hardware configuration. Although less powerful than a high-performance MS-Windows&reg; system running ADCI, the cloud-based design of ADCI_Online allows for rapid expansion of resources.&nbsp;</p>

<p>TABLE I. AWS AppStream 2.0 Instance Configurations Utilized to TesT ADCI_Online</p>

<table align=""center"">
	<tbody>
		<tr>
			<td>
			<p><strong>Name</strong></p>
			</td>
			<td>
			<p><strong>Short Name</strong></p>
			</td>
			<td>
			<p><strong>Instance Family</strong></p>
			</td>
			<td>
			<p><strong>CPU</strong></p>
			</td>
			<td>
			<p><strong>vCPUs</strong></p>
			</td>
			<td>
			<p><strong>RAM (GiB)</strong></p>
			</td>
			<td>
			<p><strong>GPU</strong></p>
			</td>
		</tr>
		<tr>
			<td>
			<p>stream.standard.medium<sup>a</sup></p>
			</td>
			<td>
			<p>standard</p>
			</td>
			<td>
			<p>General Purpose</p>
			</td>
			<td>
			<p>Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>4</p>
			</td>
			<td>
			<p>-</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>stream.standard.large</p>
			</td>
			<td>
			<p>-</p>
			</td>
			<td>
			<p>General Purpose</p>
			</td>
			<td>
			<p>Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>8</p>
			</td>
			<td>
			<p>-</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>stream.compute.large</p>
			</td>
			<td>
			<p>-</p>
			</td>
			<td>
			<p>Compute Optimized</p>
			</td>
			<td>
			<p>Intel(R) Xeon(R) CPU E5-2666 v3 @ 2.90GHz</p>
			</td>
			<td>
			<p>2</p>
			</td>
			<td>
			<p>3.75</p>
			</td>
			<td>
			<p>-</p>
			</td>
		</tr>
		<tr>
			<td>
			<p>stream.graphics.g4dn.xlarge<sup>a</sup></p>
			</td>
			<td>
			<p>G4</p>
			</td>
			<td>
			<p>Graphics G4dn</p>
			</td>
			<td>
			<p>Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.5GHz</p>
			</td>
			<td>
			<p>4</p>
			</td>
			<td>
			<p>16</p>
			</td>
			<td>
			<p>NVIDIA<sup>&reg;</sup> T4</p>
			</td>
		</tr>
	</tbody>
</table>

<p><sup>a</sup> Configurations selected for Health Canada (HC) sourced sample set analysis</p>

<p>All uploaded samples were processed on a single standard instance and on a single GPU-enabled G4 instance. Then, the samples were split into five groups of similar image count and examined simultaneously by five standard instances (A-E) run in parallel. The times required to upload and process each sample are presented in Table II. Processing of images from individual samples is not distributed between multiple instances (in contrast with the high performance computing version of ADCI<sup>(2)</sup>). Samples analyzed by each instance varied in the numbers of images they contained from 10,520 to 11,896. All samples completed with the standard AWS configuration in 46 hr 28 min and in 14 hr 24 min using the GPU-based G4 system. Sample processing with the parallelized instances ranged from 7 hr 43 min to 9 hr 20 min. Some calibration samples exceeded the number of cells required by IAEA guidelines for cytogenetic biodosimetry<sup>(22)</sup>.</p>

<p>The Data Storage and Retrieval web app uploads files concurrently in batches of 20 images. The process is repeated until all images in each batch are uploaded.. The HC 0.25 Gy calibration sample was uploaded in 11.16 minutes with a batch size of 20. The same sample was uploaded again with a batch size of 30 in 7.55 minutes. The default batch size is conservative; users connecting with high-performance computers and faster network speeds can accelerate uploads by increase this parameter.</p>

<p>The &ldquo;stream.compute.large&quot; and &ldquo;stream.standard.large&rdquo; hardware configurations processed at a rate of 12.92 and 19.89 images/min, respectively. These did not offer significant cost and performance advantages over &nbsp;the standard and G4 configurations. Results for these configurations are not described in the article due to space limitations imposed by the publisher.</p>",2021,"radiation biodosimetry, cloud computing",10.5281/zenodo.5761745,,software
ECSched: Efficient Container Scheduling on Heterogeneous Clusters,"Hu, Yang, Zhou, Huan, de Laat, Cees, Zhao, Zhiming","<p>Operating system (OS) containers are becoming increasingly popular in cloud computing for improving productivity and code porta-bility. However, container scheduling on large heterogeneous cluster is quite challenging. Recent research on cluster scheduling focuses either on scheduling speed to quickly assign resources, or on scheduling quality to improve application performance and cluster utilization. In this paper, we propose ECSched, an efficient container scheduler that can make high-quality and fast placement decisions for concurrent deployment requests on heterogeneous clusters. We map the scheduling problem to a graphic data structure and model it as minimum cost flow problem (MCFP). We implement ECSched based on our cost model, which encodes the deployment requirements of requested containers. In the evaluation, we show that ECSched exceeds the placement quality of existing container schedulers with relatively small overheads, while providing 1.1&times; better resource efficiency and 1.3&times; lower average container completion time.</p>",2021,"Container, scheduling, Cloud computing",10.1007/978-3-319-96983-1_26,,publication
The Adoption of Cloud Computing Among Private Banks Employees in Libya,"Salem Asseed Alatresh, Faisal Muftah Ahmed","<p>The adoption of cloud computing (CC) by individuals and banking institutions garnered little attention. The goal of this study is to examine the factors that influence the adoption of CC by Libyan private bank personnel. According to the findings, employees&#39; behavioral intention (BI) to adopt CC is significantly influenced by individual elements (performance expectation (PE), effort expectation (EE), social influence (SI), attitude (AT), and IT knowledge (ITK), among others. User happiness with cloud computing has been proposed to moderate the impact of individual elements on BI (SaaS). A total of 309 Libyan bank workers were randomly selected to participate in the study. Analyses of Moment Structures were used in the study (AMOS). The findings of the study show that the BI&#39;s use of CC is influenced by a variety of personal circumstances. It was partially mediated by satisfaction in BI&#39;s decision to employ CC. Employees at private banks will be more likely to use CC if they focus on their own needs.</p>",2021,"Cloud computing, banking, private banks, user satisfaction, UTAUT",10.47191/ijmcr/v9i11.05,,publication
Development on advanced technologies – design and development of cloud computing model,"Briasouli, Alexandra, Minkovska, Daniela, Stoyanova, Lyudmila","Big Data has been created from virtually everything around us at all times. Every digital media interaction generates data, from computer browsing and online retail to iTunes shopping and Facebook likes. This data is captured from multiple sources, with terrifying speed, volume and variety. But in order to extract substantial value from them, one must possess the optimal processing power, the appropriate analysis tools and, of course, the corresponding skills. The range of data collected by businesses today is almost unreal. According to IBM, more than 2.5 times four million data bytes generated per year, while the amount of data generated increases at such an astonishing rate that 90 % of it has been generated in just the last two years. Big Data have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. This paper presents a view of the BD challenges and methods to help to understand the significance of using the Big Data Technologies. This article based on a bibliographic review, on texts published in scientific journals, on relevant research dealing with the big data that have exploded in recent years, as they are increasingly linked to technology",2021,"Data, big data, knowledge mining, information explosion, data management, social networks",10.21303/2585-6847.2021.002228,,publication
IISWC 2021 Characterizing and Mitigating the I/O Scalability Challenges for Serverless Applications (Dataset and Scripts),"Rohan Basu Roy, Tirthak Patel, Devesh Tiwari","<p>As serverless computing paradigm becomes widespread, it is important to understand the I/O performance characteristics on serverless computing platforms. To the best of our knowledge, we provide the first study that analyzes the observed I/O performance characteristics -- some expected and some unexpected findings that reveal the hidden, complex interactions between the application I/O characteristics, the serverless computing platform, and the storage engines. The goal of this analysis is to provide data-driven guidelines to serverless programmers and system designers about the performance trade-offs and pitfalls of serverless I/O.</p>",2021,"Serverless Computing, I/O Variability, Cloud Computing",10.5281/zenodo.5539888,,dataset
Generating an Excerpt of a Service Level Agreement from a Formal Definition of Non-Functional Aspects Using OWL,"Rady, Mariam","If we take a look at current cloud computing services, the only quality guarantee they provide are vague Service Level Agreements(SLA). In this paper we modelled some non-functional aspects in an ontology and used this ontology as a knowledge base to generate an excerpt from a service contract. We concentrate in this excerpt on availability as it is one of the most discussed attributes in current Service Level Agreements.",2021,"QoS, SLA, contracting, non-functional aspects, OWL, ontology",10.3217/jucs-020-03-0366,,publication
A study on Time Sensitive Data Access Control,"Dr Piyush Kumar Pareek, Chaitra Y R, Saumya L","<p><em>To properly protect stored private data, the device proprietor will exchange private data with a chosen few and issue decipherment keys to them. If the user has quit, the cloud will be permitted to compensate them in re-scrambling the details and will even design new decipherment keys for current customers so that they will receive the information. Since a circulated registering environment is affecting all the different cloud servers, identical charges can&#39;t be got, and death by several of the cloud servers because of competing framework trades. The suggested approach would require learning, that will be re-scrambled, at various sections of time. The guarantee is rendered on high modern cryptography basis, with strong security emphasis, so that fine-grained knowledge gets to every business, and they don&#39;t have to get intense coordination for precision.</em></p>",2021,"decipherment keys, cloud servers, Ciphertext policy attribute-based encryption (CP-ABE), Decryption",10.5281/zenodo.4482432,,publication
"A NOVEL THIN CLIENT ARCHITECTURE WITH HYBRID PUSH-PULL MODEL, ADAPTIVE DISPLAY PRE-FETCHING AND GRAPH COLOURING","Sumalatha.M.R, Sridhar S, Satish","<p>The advent of cloud computing has driven away the notion of having sophisticated hardware devices for performing computing intensive tasks. This feature is very essential for resource-constrained devices. In mobile cloud computing, it is sufficient that the device be a thin client i.e. which concentrates solely on providing a graphical user interface to the end-user and the processing is done in the cloud. We focus on adaptive display virtualization where the display updates are computed in advance using synchronization techniques and classifying the job as computationally intensive or not based on the complexity of the program and the interaction pattern. Based on application, the next possible key-press is identified and those particular frames are pre-fetched into the local buffer. Based on these two factors, a decision is then made whether to execute the job locally or in the cloud or whether we must take the next frame from the local buffer or pull it from server. Jobs requiring greater interaction are executed locally in the mobile to reduce interaction delay. If a job is to be executed in the cloud, then the results of the processing alone are sent via the network to the device. The parameters are varied in runtime based on network conditions and application parameters to minimise the interaction delay.</p>",2021,"Cloud Computing, Mobile devices, Thin clients, Virtualization, Remote display",10.5281/zenodo.4726560,,publication
Laboratories as a Service (LaaS): Using Cloud Technologies in the Field of Education,"Pastor, Rafael, Caminero, Agustín, Sánchez, Daniel, Hernández, Roberto, Ros, Salvador, Robles-Gómez, Antonio, Tobarra, Llanos","Society has evolved in such a way that individuals are required to embrace constant improvements in order to be able to perform their jobs properly. Distance education is a solution to this problem, as it allows students to obtain practical knowledge without the space and time constraints of classical face-to-face education, thus allowing them to fit their studies into possibly tight schedules. In order to obtain practical distance education on technical topics, the use of remote laboratories becomes more of a necessity rather than just being an option. To this end, the RELATED framework has been developed in order to permit the structural development of remote laboratories. It presents a structured methodology of remote/virtual lab development and also provides common facilities, such as user management, booking, or basic visualization. In the case that a high number of laboratories and students use RELATED, handling such information becomes a major issue for the proper functionality of RELATED. These issues can be efficiently tackled using cloud technologies. This paper proposes the use of cloud technologies to enhance RELATED and describes the cloud-based architecture that is under development at UNED, including details on its software components and the algorithms needed for resource provisioning.",2021,"cloud computing, on-line education, remote laboratories, web-based services",10.3217/jucs-019-14-2112,,publication
Thinking about datification in the context of training: current challenges,Texier Jose,"<p><strong>Datification is the process of transforming data, through analysis and reorganization, into information that can be modified in any area of knowledge or discipline, but the different technological disruptive changes with the &quot;Information Era&quot;, which is currently being experienced,&nbsp; have focused professionals in Library and Information Sciences (LIS) on the most important resource to make information available to society. Therefore, this article reflects on the education of LIS professionals as the most important challenge for adapting to the endless opportunities that appear today: cloud computing services (IBM Cloud, Google Cloud Platform, Azure), machine learning, the semantic web, MOOCs, open science and open access, free access bibliographic databases, among others. In conclusion, the objective for LIS professionals is to analyze before collecting data, and this is achieved with education and not by a stroke of luck or by simply using a tool.</strong></p>",2021,"datification, LIS, challenges, opportunities, computer science, education",10.5281/zenodo.4438611,,publication
Navigating the Clouds on the Horizon: A Vision for Reproducible Hydrologic Modeling in the Cloud,"Flores, Alejandro","<p>This talk provides an overview of a new NSF project that will produce knowledge of how precipitation, snow, and runoff in the Snake River Basin will be impacted by climate variability and change, while also expanding the use of cloud computing in water science research. Although cloud computing is increasingly important in research, the water science community lacks training materials and case studies to onboard researchers to effective practices. This project addresses this tension by: (1) developing cloud computing solutions to three common computing uses in water science, (2) using those approaches to create datasets that characterize the effects of climate variability and change on hydrology in the Snake River Basin, and (3) designing and disseminating educational materials to train water scientists in the use of cloud computing. This project will produce scientific insights and datasets to help water managers prepare for climate change in the western US and prepare the next generation of water scientists in modern computing paradigms. Specific focus is paid here to how open and reproducible computing platforms are essential to this project and, importantly, can help advance co-production of knowledge and decision support for stakeholders in the realm of water management.</p>",2021,"Pangeo, Hydrology, Water Management",10.5281/zenodo.5535595,,presentation
Morphology of land Uses in Aqaba City during the Period (2000-2020),"Ibrahim Bazazo, Lama'a Mahmoud Al-Orainat, Maher Odeh Falah Al-Shamaileh","<p>The aim of this research is to identify the morphology of land usage in the city of Aqaba from the year 2000 through 2020, utilizing satellite data analysis and visualizations of topographic maps studying land uses and spatial organizations and relationships between all relevant users in the study area.</p>

<p>By relying on GIS and remote sensing software, with the aim of providing a holistic picture that contributes to identifying the current reality of land uses and future forecasting within the actual territories, the study found that the importance of integrated management based on an analysis of the morphology of land use contributed most to sustainable planning. Morphological projections relating to land uses contributed the most to correct decision making in pursuit of a more holistic planning process for the study area.</p>",2021,"City Morphology, Sustainable Planning, Spatial Analysis, Geographic Information Systems, Remote Sensing, Future Projections",10.25255/jss.2020.9.3.987.1002,,publication
A Google Earth Engine-enabled Python approach for the identification of anthropogenic palaeo-landscape features,"Brandolini, Filippo, Domingo-Ribas, Guillem, Zerboni, Andrea, Turner, Sam","<p>The necessity of sustainable development for landscapes has emerged as an important theme in recent decades. Current methods take a holistic approach to landscape heritage and promote an interdisciplinary dialogue to facilitate complementary landscape management strategies. With the socio-economic values of the ""natural"" and ""cultural"" landscape heritage increasingly recognised worldwide, remote sensing tools are being used more and more to facilitate the recording and management of landscape heritage. The advent of freeware cloud computing services has enabled significant improvements in landscape research allowing the rapid exploration and processing of satellite imagery such as the Landsat and Copernicus Sentinel datasets. This research represents one of the first applications of the Google Earth Engine (GEE)  Python application programming interface (API) in studies of historic landscapes. The complete free and open-source software (FOSS) cloud protocol proposed here consists of a Python code script developed in Google Colab, which could be adapted and replicated in different areas of the world. A multi-temporal approach has been adopted to investigate the potential of Sentinel-2 satellite imagery to detect buried hydrological and anthropogenic features along with spectral index and spectral decomposition analysis. The protocol's effectiveness in identifying palaeo-riverscape features has been tested in the Po Plain (N Italy).</p>",2021,"Multispectral analysis, Sentinel-2, Spectral decomposition, Python, Riverscape, Fluvial and Alluvial Archaeology, Landscape Archaeology, Buried features",10.12688/openreseurope.13135.2,,publication
OHEJP-RaDAR-D-JRP3-3.1 Inventory of available exposure assessment models and related data and transfer to FSK Standard,"Annemarie Käsbohrer, Jacub Fusiak, Guido Correia Carreira","<p><strong><em>1. Introduction</em></strong></p>

<p>The technological achievements of the digital age have led to an enormous increase in the number of published models. However, models are created with different programming languages. Due to a lack of harmonized model exchange formats among these tools, the exchange and usage of existing models in various software environments can be very difficult and impedes communication between researchers. The aim of this project is to provide a language independent, reproducible and user-friendly web application to facilitate the process of annotating and exchanging models.</p>

<p>The Food Safety Knowledge Markup Language (FSK-ML) has provided a solution for a harmonized model exchange format. FSK-ML defines a framework that encodes all relevant data, metadata and model scripts in an exchangeable file format. A huge advantage of this format is that it works for all models that are written in any script-based programming language. The model metadata can be shared and controlled by adhering to a metadata schema that holds vocabularies supplied by the RAKIP initiative. However, the creation of such a file can be a time consuming and difficult process. In order to increase the usage of the FSK standard, we used the web application framework Angular to develop the RaDAR model inventory. Since its backend is based on the open-source technology of KNIME, Jupyter Notebook, Binder and Thebelab, the RaDAR model inventory can support a vast majority of programming languages that run in a reproducible cloud-computing environment.</p>",2021,,10.5281/zenodo.4476609,,publication
One Health EJP - RaDAR model inventory: a user-friendly tool for annotating and exchanging models,"Fusiak, Jakub, Käsbohrer, Annemarie","<p>The lack of a harmonized model exchange formats among modelling tools impedes communication between researchers, since the exchange and usage of existing models in various software environments can be very difficult. The RaDAR model inventory aims to provide a platform to exchange models among professionals utilizing the Food Safety Knowledge Exchange (FSKX) Format (de Alba Aparicio et al. 2018) as a harmonized model exchange format. FSKX defines a framework that encodes all relevant data, metadata, and model scripts in an exchangeable file format. However, the creation of such a file can be a time-consuming and difficult process. To increase the usage of the FSK standard, we developed the RaDAR model inventory web application that targets the process of creating an FSKX file for the end user. Our inventory aims to be a user-friendly tool that allows users to create, read, edit, write, execute and compile FSKX files within the web browser. The possibility of sharing models with the public or a specific group of people facilitates collaboration and the exchange of information. Since the RaDAR model inventory is based on the open-source technology of Project Jupyter (Granger and Perez 2021), it can support nearly all relevant programming languages executed within a reproducible cloud-computing environment. The intuitive nature of the RaDAR model along with its wide range of features reduce the threshold for contribution to a harmonized model exchange format and eases collaboration. The RaDAR model inventory can be accessed at http://ejp-radar.eu.</p>",2021,"Information exchange, Modelling, Model exchange, RaDAR",10.3897/aca.4.e68936,,publication
The significance of mega sporting event on infrastructure development: A case of FIFA 2022 World Cup in Qatar,Khalifa Al-Dosari,"<p>This study sought to find how significant mega sporting events to a country are beneficial insofar as infrastructural development is concerned. The study used the 2022 FIFA World Cup in Qatar as the case study in reference. Various researches around the concept of infrastructure development due to mega sporting events were analysed in this study. The evidence of infrastructure development due to mega sporting events was also dissected and presented in the study. The research was conducted with the help of online survey questionnaires, and the data collected was analysed by using descriptive statistics as well as an OLS regression analysis. The variables measured were infrastructural developments in the country to find the significance of the 2022 FIFA World Cup. It was found that the 2022 World cup significantly affects the development of infrastructure in the country. It was therefore concluded that major sporting events are significant in the development of infrastructure of a country. It&rsquo;s recommended that the research should be used for future references in the analysis of infrastructural changes due to major sporting events.</p>",2021,"2022 FIFA World Cup, Infrastructural Developments, Regression Analysis, Mega sporting events model, SPSS",10.25255/jss.2020.9.3.1295.1319,,publication
LSB steganography strengthen footprint biometric template,Israa Mohammed Khudher,"<p>Steganography is the science of hiding secret data inside another data type as image and text. This data is known as carrier data; it lets people interconnect secretly. This suggested paper aims to design a Steganography Biometric Imaging System (SBIS). The system is constructed in a hybridization manner between image processing, steganography, and artificial intelligence techniques. During image processing techniques the system receives RGB foot-tip images and preprocesses the images to get foot-template images. Then a chain code is illustrated for personal information within the foot-template image by Least Significant Bit (LSB). Accurate recognition operation is performed by artificial bee colony optimization (ABC). The automated system was tested on a live-took about ninety RGB foot-tip images known as the cover image and clustered to nine clusters that authorized visual database. The Least Significant Bit method transforms the foot template to a stego image and is stored on a stego visual database for further use. Features database was constructed for each stego footprint template. This step converts the image to quantities data and stored in an Excel feature database file. The quantities data was used at the recognition stage to produce either a notification of rejection or acceptance. At the acceptance choice, the corresponding stego foot-tip template occurrence was retrieved, it is corresponding individual data were extracted and cluster position on the stego template visual database. Indeed, the foot-tip template is displayed. The suggested work consequence is affected by the optimum feature selection via the artificial bee colony optimization usage and clustering, which declined the complication and subsequently raised the recognition rate to 93.65&nbsp;%. This rate competes out the technique over others&rsquo; techniques in the field of biometric recognition</p>",2021,"steganography, foot-tip template, hybridization, stego image, cover image, clustering, biometrics",10.15587/1729-4061.2021.225371,,publication
The Impact of the Restaurants Services Quality on Customers Satisfaction in Aqaba Special Economic Zone Authority (ASEZA),"Nassar Mousa Nassar, Ali Falah Al Zoubi","<p>The study aims to analyze the impact of the restaurants services quality on customer&#39;s satisfaction in Aqaba Special Economic Zone Authority (ASEZA). The study sample consisted of 408 tourists; the Statistical Package for Social Sciences (SPSS) was used to process the study data. The study showed that there was a statistically significant impact (Tangibles, Responsiveness) on the satisfaction of customers in the special economic zone. The study recommends that the facilities and facilities of the restaurant should be taken into consideration. The researcher also stressed that the restaurant should have advanced equipment and equipment to assist the employee in doing business.</p>",2021,"Service Quality, Customer Satisfaction, Tourism Services, Aqaba Special Economic Zone Authority",10.25255/jss.2018.7.2.157.171,,publication
Development of software and algorithms of parallel learning of artificial neural networks using CUDA technologies,"Yaroslav Sokolovskyy, Denys Manokhin, Yaroslav Kaplunsky, Olha Mokrytska","<p><em>The object of research is to parallelize the learning process of artificial neural networks to automate the procedure of medical image analysis using the Python programming language, PyTorch framework and Compute Unified Device Architecture (CUDA) technology. The operation of this framework is based on the Define-by-Run model. The analysis of the available cloud technologies for realization of the task and the analysis of algorithms of learning of artificial neural networks is carried out. A modified U-Net architecture from the MedicalTorch library was used. The purpose of its application was the need for a network that can effectively learn with small data sets, as in the field of medicine one of the most problematic places is the availability of large datasets, due to the requirements for data confidentiality of this nature. The resulting information system is able to implement the tasks set before it, contains the most user-friendly interface and all the necessary tools to simplify and automate the process of visualization and analysis of data. The efficiency of neural network learning with the help of the central processor (CPU) and with the help of the graphic processor (GPU) with the use of CUDA technologies is compared. Cloud technology was used in the study. Google Colab and Microsoft Azure were considered among cloud services. Colab was first used to build a prototype. Therefore, the Azure service was used to effectively teach the finished architecture of the artificial neural network. Measurements were performed using cloud technologies in both services. The Adam optimizer was used to learn the model. CPU duration measurements were also measured to assess the acceleration of CUDA technology. An estimate of the acceleration obtained through the use of GPU computing and cloud technologies was implemented. CPU duration measurements were also measured to assess the acceleration of CUDA technology. The model developed during the research showed satisfactory results according to the metrics of Jaccard and Dyce in solving the problem. A key factor in the success of this study was cloud computing services.</em></p>",2021,"software, artificial neural networks, Python, PyTorch framework, CUDA, modified U-Net architecture",10.15587/2706-5448.2021.239784,,publication
Security Validation Model in Cloud Computing  Environment,"Shubhashish Goswami, Himanshu Kumar Diwedi","<p>Private, Public cloud or a unified cloud system, client&rsquo;s absence of a successful secure computable assessment techniques for handling the security circumstance of its own data foundation overall. This paper gives a quantifiable security assessment framework for various mists that can be gotten to by reliable API. The assessment framework incorporates security checking motor, security recuperation motor, secure computable assessment system, graphical presentation segment &amp; so on. Secure assessment system makes out of many assessment components comparing various fields, for example, figuring, stockpiling, organize, support, application security and so forth. Every component is doled out 3 tuples on the liabilities, score &amp; fix strategy. Framework receives &quot;1 vote&quot; system for a field to check its point &amp; includes synopsis as overall score, &amp; to make high security. We implement the computable assessment for various cloud environment clients dependent on the G Cloud phase. It displays active security examining for one or different clouds with pictorial diagrams &amp; clients to adjust arrangement, expand activity &amp; fix liabilities, in order to increase secureness of cloud assets.</p>",2021,"security, quantifiable evaluation, secure validation, secure view, cloud computing",10.35940/ijeat.B4233.029320,,publication
2D Transformations Analyzed by Both Column  Vector and Row Vector Synthesis,"Shweta Chaku, Amrita Bhatnagar","<p>The 2D aspects of Computer Graphics such as vector primitives and 2D transformations are important in creating 2D content. The Transformation are the effective means of shifting or changing the dimensions and orientations of images in the most effective way. If we fail to transform the object in terms of displacement ,enlargement, orientation, we may land up in creating something that is distorted and processessing a distorted object is not acceptable The usual practice of defining transformations is straight forward. The transformed object can be obtained by coupling original object with the transformation vectors .The main challenge is how to evaluate it. The usual practice is standard Column Vector form. The alternative Row Vector Form is also known approach but what matters is the sequence of operations that make these both approaches worth mentionin .While doing so our analysis on 2D content keeps our knowledge flawless and takes it a step further as far as Image Processing is concerned. Such analytical study is very vital since most of the content created, acquired, reproduced, and visualized in 2D needs to be mapped on to 3D.This paper describes the transformations(Translation,Scaling and Rotation) in the both Column and Row Vectar Approach. This paper aims in providing a clear sequence of calculations which differ in both approaches</p>",2021,"2DTransformations, Homogeneous Coordinates, Rotation Scaling, Translation, Reflection",10.35940/ijeat.C6149.029320,,publication
Hybrid Multi-Cloud based Disease Prediction  Model for Type II Diabetes,"M. Durgadevi, R. Kalpana","<p>Advancements in health informatics pave the way to explore new medical decision making systems which are characterized by an exponential evolution of knowledge. In the medical domain, disease prediction has become the centre of research with the increasing trend of healthcare applications. The predictive knowledge for the diagnosis of disease highly depends on the subjective knowledge of the experts. So the development of a disease prediction model in time is essential for patients and physicians to overcome the problem of medical distress. This paper explores a hybrid approach (Cooperative Ant Miner Genetic Algorithm) for classifying the medical data. Three benchmarked Type II diabetic datasets (US, PIMA, German) from the UCI machine learning repository were used to analyze the effectiveness of the disease prediction model. The devised classification algorithm with a Soft-Set approach was deployed in a Multi-Cloud environment for enhancing the storage and retrieval of data with reduced response and computation time. The cooperative classification algorithm in the cloud database distinguishes the diseased cases from the normal ones .The soft set theory analyzes the severity of the diseased cases by calculating the percentage of diabetic risk using soft intelligent rules and stores them in a separate knowledge base. Thus the proposed model serves as a suitable tool for eliciting and representing the expert&rsquo;s decision which aids in prediction of Type II diabetic risk percentage leading to the timely treatment of patients.</p>",2021,"Disease Prediction, Ant Miner Algorithm, Genetic Algorithm, Soft Sets, Multi cloud storage",10.35940/ijeat.C5680.029320,,publication
Mobility services data models for open and inclusive MaaS infrastructures,"Salamanis, Athanasios, Ioakeimidis, Theodoros, Gkemou, Maria, Kehagias, Dionysios, Tzovaras, Dimitrios","<p>Over the recent years, the vast variety of widely accessible cloud computing services along with the need to<br>
combine transportation services either from public or private providers, have led to the rise of the Mobility<br>
as a Service (MaaS) concept. The main feature of MaaS is that it gives users access to a set of heterogeneous<br>
transportation services from a single access point (i.e., an app). The ever-increasing adoption of MaaS<br>
by service providers introduces a variety of new business models and technologies that can successfully<br>
support the design and deployment of MaaS services.</p>",2021,"MaaS, JSON schema, Ontology, OWL, KPI, Cyclomatic complexity, Halstead metrics, Maintainability index, Technical debt",10.5281/zenodo.4434305,,publication
