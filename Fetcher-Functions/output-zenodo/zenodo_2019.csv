title,authors,abstract,year,keywords,doi,url,type
Semantic web and knowledge graphs as an educational technology of personnel training for nuclear power engineering,"Telnov, Victor, Korovin, Yuri","<p>The technologies of knowledge representation and inference in an artificial intelligence system focused on the domain of nuclear physics and nuclear power engineering are considered. The possibilities of description logics and graph databases of nuclear knowledge for the generation of cognitive hypotheses, using in addition to deduction and other ways of reasoning, such as inductive inference and reasoning based on analogies, are discussed. The use of adequate description logic and measures of semantic similarity is substantiated. Interactive visual navigation and reasoning on the knowledge graphs are performed by means of special retrieval widgets and the smart RDF browser. Operations with semantic repositories are implemented on cloud platforms using SPARQL queries and RESTful services. The proposed software solutions are based on cloud computing using DBaaS and PaaS service models to ensure scalability of data warehouses and network services. Example of use of the offered technologies and software has been given.</p>",2019,"Nuclear education, semantic web, knowledge graph, cloud computing",10.3897/nucet.5.39226,,publication
PATTERNS IN FORMING THE ONTOLOGY-BASED ENVIRONMENT OF INFORMATION-ANALYTICAL ACTIVITY IN ADMINISTRATIVE MANAGEMENT,"Oleksandr Nesterenko, Oleksandr Trofymchuk","<p>A new paradigm of the formation of the environment of informational-analytical activity in administrative management based on ontologies was proposed. It was shown that application of this approach makes it possible to formalize domain area and structure the information necessary for analytical activity. It was established that the use of ontological descriptions in the technological chain of analytical activity ensures dynamic formation for the analysis of the respective sets of the criteria based on the use of the properties of concepts of the domain areas, by which appropriate decisions are made. It is noted that the process of solving an analytical problem may represent a certain sequence of ordered tautologies, each of which inherits all the properties of the concepts that make up the tautology that directly precedes it. In turn, this sequence determines the set of possible taxonomies as functional components of the operational environment of informational-analytical activity. To support the work of an analyst, it is proposed to apply the hierarchies of ontologies from the upper level to the subject ontologies, including the intermediate level of the ontology core. The ontology core is expanding through ontological linking of ontology classes to such information resources as classifiers. Correctness and adequacy of such decision is proved by the use of this paradigm to solve the problem of administrative monitoring of socio-economic development of the regions of a country from the state level to local self-government</p>",2019,"informational-analytical system, management body, administrative management, information resources, ontology, taxonomy, classifier",10.15587/1729-4061.2019.180107,,publication
Interoperability of EO cloud computing services,"Schramm, Matthias, Pebesma, Edzer, Mohr, Matthias, Jacob, Alexander, Dries, Jeroen, Foresta, Luca, Neteler, Markus","<p>This document represents all presentations, held during ESA Phiweek at ESRIN (Frascati / IT) at the side event &quot;Interoperability of EO Cloud Computing Services With the openEO API&quot;.</p>

<p>Following side event&#39;s description was published by the Phiweek&#39;s organizers.</p>

<p><em><strong>Planned outcome of openEO</strong></em></p>

<p><em>Copernicus and other novel Earth Observation (EO) programmes are generating data of unprecedented quality and volume. To scope with this, Petabyte-scale EO data centres and cloud computing services have been set up over the last decade, resulting in a variety of customised EO processing platforms. Nowadays, available services are ranging from Data-as-a-Service (DaaS) to Platform-as-a-Service (PaaS), are tailored by their user&#39;s needs and are often closed-source up to different degrees (e.g. DIAS, VITO, EODC, Copernicus Global Land Service, Google Earth Engine, Amazon Web Services, &hellip;). This heterogeneity, the diverse user demands as well as the emergence of the various service offerings makes it currently difficult for end users to compare results from different platforms, or costs of offerings.</em></p>

<p><em>openEO is a user-driven open source project that develops an API (a language) for communication between users of diverse programming environments (e.g. web browser, Jupyter notebooks, RStudio) and various EO service providers. It aims at cross-cloud platform interoperability (i) to merge heterogeneous user communities of different platforms, now developing combinable workflows and thus to enable more holistic and specialised cloud computing approaches in the EO sector, (ii) to allow users an easier switch between service providers, (iii) to implement cross-platform communication strategies and thus to develop de-centralised workflows, considering the capabilities of individual service providers, and (iv) to allow the comparison of the platforms processing results, capabilities and pricing. For the communication between clients and EO cloud providers, the workflows&#39; commands are chained to standardised process graphs in a JSON format and transferred via web request to an interface at the service provider. The jobs are then translated by the openEO back-end to the platform&#39;s local syntax and are either executed as batch jobs, or lazy evaluated as part of secondary web services for web-based access or executed synchronously in case of lightweight jobs. With openEO, former needed many-to-many connections between clients and the cloud providers are reduced to many-to-one connections.</em></p>

<p><em>The openEO API entails processes from all aspects of the EO data life cycle. It prepares the data as a &#39;virtual data cube&#39; &ndash; independently of the back-ends storage data structure. The openEO processes support following data manipulation: (i) EO data can be subsetted, (ii) dimension can be removed or added by computation, (iii) resampling and aggregation processes allow e.g. reprojecting or rescaling of EO data, (iv) pixel-based math processes are available (e.g. sorting algorithms, unary functions), and (v) the capability of processing user-defined functions (UDFs) will allow openEO to meet extremely specialised user demands. All currently available processes are listed and described at https://processes.openeo.org.</em></p>

<p><em>While openEO is designed as language neutral, clients for Python, R, and JavaScript are currently developed. Compatible service providers are momentarily VITO, EODC, mundialis, Sinergise, EURAC Research, JRC and Google Earth Engine; further connections are planned. The openEO API is conceived to be used as a template for further service providers to easily connect to the freely available API (https://github.com/Open-EO/). A stable version was released in July 2019, which will be demonstrated in this workshop.</em></p>

<p><em><strong>Aim of the workshop</strong></em></p>

<p><em>This workshop will provide participants with an overview of using openEO on different cloud platforms. Service providers will gain insights in implementing the open source API on their system and thus connecting to the joint user community.</em></p>

<p><em>The side event is split into 2 sessions of 2 hours each, showing live demonstrations and covering various aspects of the openEO API. The first session will concentrate on the clients perspective, demonstrating its use via Python, R, and web interfaces on various service providers (VITO, Mundialis, EURAC Research, Google Earth Engine). The second session will deal with the installation of openEO instances on service provider&rsquo;s premises and chosen standards of the openEO API.</em></p>

<p>&nbsp;</p>",2019,,10.5281/zenodo.3453789,,presentation
BIG DATA MANAGEMENT AND CLOUD  COMPUTING WITH POTENTIALITIES IN  ACADEMIA: AN INDIAN SCENARIO,"P. K. Paul, A. Bhuimali, P Sreeramana Aithal","<p>Development &nbsp;and &nbsp;progress &nbsp;are &nbsp;truly &nbsp;depends &nbsp;on &nbsp;knowledge &nbsp;dissemination &nbsp;and &nbsp;cultivation.&nbsp;<br>
Emerging &nbsp;technologies are &nbsp;the key pillar &nbsp;for complete &nbsp;industrial solutions and ultimately &nbsp;for&nbsp;<br>
the building of solid industrial society. And it is important step for reaching knowledge society&nbsp;<br>
development. &nbsp;Educational &nbsp;programs &nbsp;and &nbsp;courses &nbsp;play &nbsp;a &nbsp;greater &nbsp;role &nbsp;for &nbsp;such &nbsp;development.&nbsp;<br>
Social &nbsp;development &nbsp;is &nbsp;purely &nbsp;related &nbsp;economical &nbsp;progress &nbsp;and &nbsp;that &nbsp;is &nbsp;related &nbsp;with &nbsp;the&nbsp;<br>
educational &nbsp;delivery.</p>",2019,,10.5281/zenodo.3407324,,publication
"Advances in Cloud Computing, Data Science and Big Data Analytics",Mao Ito,"<p>Graphical Model Lab: Towards the Development of Graphical Modelling Open Source SaaS<br>
&nbsp;</p>",2019,,10.5281/zenodo.2576672,,publication
A Bi- Objective Workflow Application Scheduling In Cloud Computing Systems,Yalda Aryan,"<p><strong>ABSTRACT </strong></p>

<p>The task scheduling is a key process in large-scale distributed systems like cloud computing infrastructures which can have much impressed on system performance. This problem is referred to as a NP-hard problem because of some reasons such as heterogeneous and dynamic features and dependencies among the requests. Here, we proposed a bi-objective method called DWSGA to obtain a proper solution for allocating the requests on resources. The purpose of this algorithm is to earn the response quickly, with some goal-oriented operations. At first, it makes a good initial population by a special way that uses a bidirectional tasks prioritization. Then the algorithm moves to get the most appropriate possible solution in a conscious manner by focus on optimizing the makespan, and considering a good distribution of workload on resources by using efficient parameters in the mentioned systems. Here, the experiments indicate that the DWSGA amends the results when the numbers of tasks are increased in application graph, in order to mentioned objectives. The results are compared with other studied algorithms.</p>

<p><br>
<strong>Original Source </strong><strong>URL :</strong><strong>&nbsp;<a href=""http://airccse.org/journal/ijite/papers/3214ijite06.pdf"">http://airccse.org/journal/ijite/papers/3214ijite06.pdf</a></strong></p>

<p><strong>For more </strong><strong>details :</strong><strong>&nbsp;<a href=""http://airccse.org/journal/ijite/vol3.html"">http://airccse.org/journal/ijite/vol3.html</a></strong></p>",2019,"Cloud computing, Heterogeneous distributed computing systems, market oriented systems, Workflow scheduling, Genetic Algorithm",10.5281/zenodo.2575352,,publication
Cloud Computing Based Knowledge Mapping Between Existing and Possible Academic Innovations—An Indian Tecno -Educational Context,"P K Paul, Vijender Kumar Solanki, P Sreeramana Aithal",<p>Cloud Computing Based Knowledge Mapping Between Existing and Possible Academic Innovations</p>,2019,,10.5281/zenodo.3593902,,publication
Review Paper On An Efficient Encryption Scheme In Cloud Computing Using ABE,"Rutuja G. Kaple, Prof. S. B. Rathod","<p>Security for the data which is stored on the cloud by user is very important issue. User may expect some security for their data from the cloud service provider, there can be serious issues concerning data security between user and service provider. To solve this kind of issues, we can use third party as an auditor. Here we have analyzed different ways to ensure secure data storage in cloud. We are going to provide the security to the user&#39;s data by using encryption technique. For this we are using the Advanced Encryption Standard algorithm for encryption and decryption. But when Cloud Service Provider has both encryption and decryption keys, there is threat to security and privacy of data. CSP may pass the user data without user&#39;s knowledge. For auditing we are introducing Third Party Auditor. Here the data will be encrypted at user side and will be in encrypted form over network and to TPA. TPA will verify the data before storing it on the cloud. There are large numbers of users of cloud computing who are accessing and modifying the data and they need the reliable service provider who can provide complete security for their data. So the TPA will audit the data and check the data integrity of client&#39;s data. Hence user will have more elaborated view over his data privacy. Rutuja G. Kaple | Prof. S. B. Rathod &quot;Review Paper On An Efficient Encryption Scheme In Cloud Computing Using ABE&quot; Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: https://www.ijtsrd.com/papers/ijtsrd10915.pdf</p>",2019,"Computer Network, Cloud Computing, Attribute based encryption, Key policy, ciphertext policy",10.31142/ijtsrd10915,,publication
Review on CDSS implementation with CDA generation and integration for health information exchange in cloud,"Pooja N. Umekar, Dr. H R. Deshmukh, Prof O A. Jaisinghani","<p>Successful Deployment of EHR helps to improve patient safety and quality of care, but it has the prerequisite of interoperability between health information exchange at different hospital. The clinical document architecture developed HL7 is core document standard to ensure such interoperability and propagation of this document. Unfortunately, hospitals are reluctant to adopt interoperable HIS due to its deployment cost except for in a handful countries. A problem arises even when more hospital s start using the CDA document format because the data scattered in different documents are hard to manage. CDA document generation and integration Open API service based on cloud computing through which hospitals are enabled to conveniently generate CDA document per patient into a single CDA document and physician and patients can browse the clinical data in chronological order. CDA system of CDA document generation and integration is based on cloud computing and the service is offered in Open API. A clinical decision support system CDSS is a health information technology system that is designed to provide physicians and other health professionals with clinical decision support CDS , that is assistance with clinical decision -making tasks. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence &quot;Clinical decision support systems link health observations with health knowledge to influence health choices by clinicians for improved health. Our system will implement CDSS clinical decision support system it the system looking towards the clinical decision support from existing CDA system provided information. CDSS will help the doctor&#39;s do the diagnosis of the patient from the existing CDA system. So that the time will be saved of the patient or if the patient is unable to bring or collect the health record the doctor&#39;s will be able to detect the particular disease. Pooja N. Umekar | Dr. H R. Deshmukh | Prof O A. Jaisinghani &quot;Review on CDSS implementation with CDA generation and integration for health information exchange in cloud&quot; Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-3 , April 2018, URL: https://www.ijtsrd.com/papers/ijtsrd11130.pdf</p>",2019,"Computer Engineering, Health information exchange, CDA, cloud computing",10.31142/ijtsrd11130,,publication
THE USE OF CLOUD TECHNOLOGIES IN THE EDUCATIONAL PROCESS OF HEI,"Rekun, O.","<p>Informatization of the education takes one of the main places among many directions of its development. Lately, one of the key trends in the IT industry is cloud technologies. The use of cloud technologies in the educational process can be done in two ways: learning cloud technologies and using cloud technologies by learning.</p>

<p>Cloud technologies in the educational process of HEI are analyzed within the article. Advantages and disadvantages of introducing cloud technologies in higher education institutes are determined, software review is done. Existing models of maintenance of cloud technologies are considered, the choice of presented models is substantiated. A set of knowledge, learning skills that enable a modern expert to work effectively with information in global computer networks, share it with colleagues is specified. A review of Internet platforms offering cloud technologies, in order to identify among them the most optimal for the educational objectives, is performed. Google-services and their potential for conducting classes in institutions of higher education are characterized.</p>

<p><strong>Methodology.</strong> Analysis and synthesis of scientific publications, psycho-pedagogical and methodical literature, internet sources, ascent from abstract to concrete in order to reveal the main definitions of the problem under study.</p>

<p><strong>Scientific novelty.</strong> Theoretical aspects of the use of cloud technologies in the educational process for higher education institutes have been identified, defined and generalized.</p>

<p><strong>Conclusions.</strong> The use of cloud technologies is a promising direction, which allows increasing the efficiency of the educational process and reducing the cost of its implementation. The proposed decision of the organization of the educational process based on building a private learning cloud adds a number of innovative methods, in comparison with the traditional model of education and can be successfully implemented in the modern educational system, as well as in creating effective tools for organizing research activities.</p>",2019,"""cloud"", ""cloud technologies"", educational process",10.5281/zenodo.3252294,,publication
CDSS implementation with CDA generation and integration for health information exchange in cloud,"Pooja N. Umekar, Dr. H R. Deshmukh, Prof. O. A. Jaisinghani, Prof S.V. Khedkar","<p>Electronic health record helps to improve the safety and quality care of every individual patient details, that to be stored in various hospital through health information exchange. The clinical document architecture CDA developed by Health level seven HL7 is core document standard that ensure interoperability of the document. Hospitals are reluctant to adopt interoperable hospital information system due to its deployment cost except for in a handful countries. A problem arises even when more hospitals start using the CDA document format because the data scattered in different documents are hard to manage. CDA document generation and integration Service based on cloud computing through which hospitals are enabled to conveniently generate CDA document per patient into a single CDA document and physician and patients can browse the clinical data in chronological order. To improve the accuracy and speed of diagnosis, health care system is important to provide the faster and efficient way. A clinical decision support system CDSS is a health information technology system that is designed to provide physicians and other health professionals with clinical decision support CDS , that is assistance with clinical decision -making tasks. The system is designed by using various data mining techniques to assist the diagnosis of patient&#39;s symptoms. Our system is designed with the help of Na&Atilde;&macr;ve Bayesian classification technique which has overcome the various data mining technique to diagnose the patient symptoms. The Na&Atilde;&macr;ve Bayesian classification technique provide the diagnosis of disease with the help of symptoms occurs to the patient .&quot;Clinical decision support systems link health observations with health knowledge to influence health choices by clinicians for improved health. Our system implement CDSS clinical decision support system looking towards the system CDSS clinical decision support system diagnose the diseases of the patient and also the CDA is generated which will be in XML form and also it can be integrated through various platforms. With the help of this system the time of patient would be saved and accurate diagnoses of the patient is done. Pooja N. Umekar | Dr. H R. Deshmukh | Prof. O. A. Jaisinghani | Prof S.V. Khedkar &quot;CDSS implementation with CDA generation and integration for health information exchange in cloud&quot; Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-4 , June 2018, URL: https://www.ijtsrd.com/papers/ijtsrd14127.pdf</p>",2019,"Computer Engineering, Health information exchange( HL7) CDA, cloud computing, software as a service, CDSS",10.31142/ijtsrd14127,,publication
"Multi-access edge computing: open issues, challenges and future perspectives","Sonia Shahzadi, Muddesar Iqbal, Tasos Dagiuklas, Zia Ul Qayyum","<p>Latency minimization is a pivotal aspect in provision of real time services while adhering to Quality of Experience (QoE) parameters for assuring spectral efficiency. Edge Cloud Computing, being a potential research dimension in the realm of 5G networks, targets to enhance the network efficiency by harnessing effectiveness of both cloud computing and mobile devices in user&rsquo;s proximity. Keeping in view the far ranging impact of Edge Cloud Computing in future mobile generations, a comprehensive review of the prevalent Edge Cloud Computing frameworks and approaches is presented with a detailed comparison of its classifications through various QoS metrics (pertinent to network performance and overheads associated with deployment/migration). Considering the knowledge accumulated, procedures analysed and theories discussed, the paper provides a comprehensive overview on sate-of-the-art and future research directions for multi-access mobile edge computing.</p>",2019,,10.1186/s13677-017-0097-9,,publication
Research Object Composer: A Tool for Publishing Complex Data Objects in the Cloud,"Anita de Waard, Marina Soares E Silva","<p>In recent years there has been a surge of efforts in bioinformatics to move data towards a &lsquo;Data Commons&rsquo;, (see e.g. [1 &ndash; 3]) that intends to &ldquo;collate data with cloud computing infrastructure and commonly used software services, tools, and applications to create biomedical resources for the large-scale management, analysis, harmonization, and sharing of biomedical data&rdquo; [3]. To create such a shared knowledge infrastructure, it is necessary that data and software, previously installed or hosted locally, become globally accessible, and made Findable, Accessible, Interoperable and Reusable [4]. As a first step to finding data and software, a unique, global, persistent identifier system is required, which is flexible enough to accommodate the multifarious inputs and outputs which a Data Commons can produce.</p>",2019,"Research Objects, Cloud-based workflows, Mendeley Data",10.5281/zenodo.3382263,,publication
Monitoring of the development of information infrastructure in Ukraine,Roman Prav,"<p><em>The object of research is the information infrastructure of Ukraine. One of the most problematic places is the lack of an assessment and monitoring system for the development of information infrastructure. It was revealed that the main drawbacks of the existing monitoring indicators are the lack of consideration of the level of information threats. Assessment of the development of information infrastructure takes place without taking into account the degree of information security of its objects.</em></p>

<p><em>During the study, methods of system analysis were used to assess the indicators of the development of information infrastructure in the context of the level of information threats. Meta-analysis of scientific works and normative legal acts is used to systematize the scientific provisions on research issues.</em></p>

<p><em>It is established that the system of indicators used for assessing information security is most often used, including the indicators of the level of development of information and communication technologies in the context of the main subjects of the information infrastructure. This is due to the availability of information.</em></p>

<p><em>Due to the developed system of assessment and monitoring of the development of information infrastructure, it is possible to obtain knowledge about the level of information threats and security. Compared to similar well-known approaches to evaluation, this provides a number of benefits. In particular, it is possible to identify critical infrastructure objects that are characterized by the highest level of exposure to information threats. This approach has proved to be necessary to ensure the protection of information in the private sector of the information sphere.</em></p>

<p><em>The assessment of the state of the information infrastructure on the basis of the developed monitoring system made it possible to identify a number of trends. The development of the information infrastructure of enterprises and technologies in the information sphere is fast-paced. This is due to the increasing level of computerization, technological re-equipment, the use of social media companies and the &laquo;big data&raquo; analysis. One of the factors is the rapid spread of cloud technologies and computing. At the same time, it enables to automate the business processes of processing and analysis of information. However, on the other hand, it serves as a source of threats to internal information and information infrastructure.</em></p>",2019,"information threats, information security, monitoring system, indicators of information threats, protection of personal information",10.15587/2312-8372.2019.169592,,publication
cTuning.org: systematizing tuning of computer systems using crowdsourcing and statistics,Grigori Fursin,"<p>Continuing innovation in science and technology is vital for our society and requires ever-increasing computational resources. However, delivering such resources has become intolerably complex, ad-hoc, costly and error-prone due to an enormous number of available design and optimization choices combined with the complex interactions between all software and hardware components. Auto-tuning, run-time adaptation, and machine learning based approaches have been demonstrating good promise to address above challenges for more than a decade but are still far from the widespread production use due to unbearably long exploration and training times, lack of a common experimental methodology, and lack of public repositories for unified data collection, analysis and mining.<br>
<br>
In this talk, I presented a long-term holistic and cooperative methodology and infrastructure for systematic characterization and optimization of computer systems through unified and scalable repositories of knowledge and crowdsourcing. In this approach, multi-objective program and architecture tuning to balance performance, power consumption, compilation time, code size and any other important metric&nbsp;are transparently distributed among multiple users while utilizing any available mobile, cluster or cloud computing services. Collected information about program and architecture properties and behavior is continuously processed using statistical and predictive modeling techniques to build, keep and share only useful knowledge at multiple levels of granularity. Gradually increasing and systematized knowledge can be used to predict most profitable program optimizations, run-time adaptation scenarios and architecture configurations depending on user requirements. I also presented a new version of the public, open-source infrastructure and repository (cTuning3 aka Collective Mind) for crowdsourcing auto-tuning using thousands of shared kernels, benchmarks and datasets combined with online learning plugins. Finally, I also discussed encountered challenges and some future collaborative research directions on the way towards Exascale computing.</p>",2019,"open science, collaborative research, efficient systems, crowdsourcing autotuning, self-optimizing systems, brain-inspired computing",10.5281/zenodo.2544230,,presentation
Workflow environments for advanced cyberinfrastructure platforms,"Badia, Rosa Maria, Ejarque, Jorge, Lezzi, Daniele, Lordan, Francesc, Conejero, Javier, Alvarez Cid-Fuentes, Javier, Becerra, Yolanda, Queralt, Anna","<p>Progress in science is deeply bound to the effective use of high-performance computing infrastructures and to the efficient extraction of knowledge from vast amounts of data. Such data comes from different sources that follow a cycle composed of pre-processing steps for data curation and preparation for subsequent computing steps, and later analysis and analytics steps applied to the results. However, scientific workflows are currently fragmented in multiple components, with different processes for computing and data management, and with gaps in the viewpoints of the user profiles involved. Our vision is that future workflow environments and tools for the development of scientific workflows should follow a holistic approach, where both data and computing are integrated in a single flow built on simple, high-level interfaces. The topics of research that we propose involve novel ways to express the workflows that integrate the different data and compute processes, dynamic runtimes to support the execution of the workflows in complex and heterogeneous computing infrastructures in an efficient way, both in terms of performance and energy. These infrastructures include highly distributed resources, from sensors and instruments, and devices in the edge, to High-Performance Computing and Cloud computing resources. This paper presents our vision to develop these workflow environments and also the steps we are currently following to achieve it.</p>",2019,,10.1109/ICDCS.2019.00171,,publication
'The Last Mile': The registry behind the identifier,"Hardisty, Alex, Lannom, Larry, Koureas, Dimitris, Addink, Wouter, Weiland, Claus","<p>Preserved specimens in natural science collections have lifespans of many decades and often, several hundreds of years. Specimens must be unambiguously identifiable and traceable in the face of changes in physical location, changes in organisation of the collection to which they belong, and changes in classification. When digitizing museum collections, a clear link must be maintained between the physical specimen itself and the information digitally representing that specimen in cyberspace. The idea of a Natural Science Identifier (NSId) as a neutral, unique, universal and stable long-term persistent identifier (PID) of a 'Digital Specimen' is central to museums' ambitions for widening access. An NSId allows easy identification and referencing of specific Digital Specimens, regardless of type, location, owner or user. It provides a digital doorway to physical specimens through which services for arranging loans and visits can be accessed, as well as opening the door to innovative services for manipulating specimens' information directly; for work reliant upon discovery of related third-party information; and for demanding 3D modelling and visualization of specimens. Because the work takes place within e-Infrastructures/Cyberspace, new possibilities for analysing hundreds of thousands of specimens simultaneously are opened by exploiting large-scale cloud computing capacity and deep mining/machine learning, for example.</p>
  <p>There are several established identifier mechanisms that could be used as a basis for NSId, but some variant of Handles is most appropriate over the very long-term because of their neutrality, resistance to change and sustainability. Adopted uses of the Handle system include identification of journal articles and datasets in education and research (using Digital Object Identifiers); film and television programme assets in the entertainment sector; financial derivatives; and for international shipping and construction.</p>
  <p>Aside from being stable and sustained over time, an essential requirement of a global PID mechanism is independence from the museums/institutions assigning identifiers. NSIds are opaque insofar as no information can or should be inferred solely by inspecting the identifier. Stakeholders change, collections move, and organisations evolve, merge or disappear. Even designations and descriptions of specimens and collections can change. Information should only be revealed when the identifier is resolved via a neutral index.</p>
  <p>One can debate the most appropriate instantiation of the Handle system but this is not useful. Relevance, ease of use and added-value of the supporting 'NSId Registry' (NSIdR) – the index of the different kinds of natural science object and their relations – are the decisive factors. This can be seen from the example of the Entertainment Identifier Registry (EIDR) founded by the major motion picture studios to create a reliable way to identify and track film and TV content distribution. Focus on the object model, promotional branding and value perception in the target user segment are the critical factors for success. Providing such a registry, seamlessly coupled to work practices and language of the professionals addresses the last mile challenge (Koureas et al. 2016).</p>
  <p>From specimens, class characteristics, storage containers and collections, to specific identifications, images, naming, literature references and more, the NSIdR's triple-hierarchy object model, rooted in OBO Foundry's Biological Collections Ontology, is the key to persistently identifying, relating and indexing the entire range of collection objects of interest to scientists and others working in the bio and geo realms. The NSIdR 'knowledge graph', interoperable with other identifier schemes, supports novel first- and third-party value-add services such as arranging loans and visits, curation and annotation, and machine-learning for relationship discovery and pattern exploration.</p>",2019,"persistent identifier, registry, Digital Object Architecture, handle",10.3897/biss.3.37034,,publication
Energy Efficient For Cloud Based GPU Using DVFS With Snoopy Protocol,"Prof. Avinash Sharma, Anchal Pathak","<p>GPU design trends show that the register file size will continue to increase to enable even more thread level parallelism. As a result register file consumes a large fraction of the total GPU chip power. It explores register file data compression for GPUs to improve power efficiency. Compression reduces the width of the register file read and writes operations, which in turn reduces dynamic power. This work is motivated by the observation that the register values of threads within the same warp are similar, namely the arithmetic differences between two successive thread registers is small. Compression exploits the value similarity by removing data redundancy of register values. Without decompressing operand values some instructions can be processed inside register file, which enables to further save energy by minimizing data movement and processing in power hungry main execution unit. Evaluation results show that the proposed techniques save 25 of the total register file energy consumption and 21 of the total execution unit energy consumption with negligible performance impact. Performance and energy efficiency are major concerns in cloud computing data centers. More often, they carry conflicting requirements making optimization a challenge. Further complications arise when heterogeneous hardware and data center management technologies are combined. For example, heterogeneous hardware such as General Purpose Graphics Processing Units GPGPUs improved performance at the cost of greater power consumption while virtualization technologies improve resource management and utilization at the cost of degraded performance. Prof. Avinash Sharma | Anchal Pathak &quot;Energy Efficient For Cloud Based GPU Using DVFS With Snoopy Protocol&quot; Published in International Journal of Trend in Scientific Research and Development (ijtsrd), ISSN: 2456-6470, Volume-2 | Issue-6 , October 2018, URL: https://www.ijtsrd.com/papers/ijtsrd18412.pdf</p>",2019,"Computer Network, GPU, Energy Consumption, Data Redundancy, Compression",10.31142/ijtsrd18412,,publication
"Bioscience Data Literacy At The Interface Of The Environment, Human And Wildlife: One Health-centred education, research and practice perspectives in Rwanda","Karame, Prosper, Gashakamba, Faustin, Dushimiyimana, Valentine, Nshimiyimana, Ladislas, Ndishimye, Pacifique","<p>Advances in information technology have led to the availability of state-of-the-art technologies which in turn have been enabling the generation of unprecedented amounts of complex, structured or unstructured data sets that are sometimes difficult to process using conventional techniques. In particular, handling these large scale data in terms of collection, and aggregation, synthesis and analysis, interpretation, reporting, sharing and archiving processes, and interpreting them into descriptive models and enable effective interpretation requires continued development of robust computational models, algorithms and interoperable analytical frameworks (Hampton et al. 2017). This also involves the vital availability of data management expertise and reflects an imperative need for data science professionals, especially in the context of generating the most informative data for use and drive evidence-based decisions. Considering this, Rwanda has been fueling its economic transformation agenda, and, while this solely depends on natural resources exploitation, the scenario has led to critically concerning anthropogenic threats and unprecedented environmental vulnerability. Acknowledging the urgency to achieve its development needs while at the same time safeguarding the environmental sustainability, Rwanda has been promoting technology-enabled systems and approaches for sustainable management of environment and natural resources. </p>
  <p>Learning from global initiatives, Rwanda's journey targets the effective use of technology-supported systems and data science expertise to effectively drive management and decision making needs in environmental management, health research systems and biodiversity conservation planning (Karame et al. 2017). Rwanda champions the adoption and effective use of technology towards delivering its vision of knowledge-based economy. A particular emphasis relates to streamlining the education, research and application of technology-supported systems and platforms and strengthening their effective use. From a practical One Health perspective, Rwanda has been bridging inter-sectoral gaps related to joint planning and resource sharing for informed decision processes. This One Health concept emphasizes the interconnection of the health of human, animals and ecosystems and involves the applications of multidisciplinary, coordinated, cross-sectoral collaborative efforts to attain optimal health for people, animals and the environment (Buttke et al. 2015). One Health constitutes a promising approach in the advancement of biosciences. For example, big data and ecological and digital epidemiology analysis has led to promising progress beyond the traditional transdisciplinary conservation medicine approach, and One Health is now driving solutions to major conservation and health challenges.</p>
  <p>This paper aims to explore the perspectives of solving challenges in handling heterogeneous data and sources of uncertainty, the progress and feasibility of adopting (or developing, adapting and customizing) open code- and data-sharing platforms, and integrating the application of flexible statistical models and cloud-computing, all within the confines of limited resources. Africa needs to engage in data science to build and sustain capacity and to effectively use acquired knowledge and skills.  Further, Africa can strategically align and tailor existing technology data science platforms to the unique context of this continent. It is time to assess the boundaries, explore new horizons, and reach beyond the limits of current practice in order to enable researchers to get the most from generated data. We envision a long-term integrative and digital approach to handling and processing health, environment, and wildlife data to mark the beginning of our journey forward.</p>",2019,"bioscience, data-literacy, one-health, environment, health, wildlife management",10.3897/biss.3.39312,,publication
Innovative management of the refugee integration model through self-employment,"Nur Akarcay, Yeliz, Ochoa-Daderska, Renata","<p>The objective of this Project is to find a digital tool that solves some of the important needs of youth refugees, contributing to the well-being and integration of them in Europe.&nbsp; The design of the project has been based on a previous diagnosis of each partner Institution.&nbsp; This study has allowed us to identify the strengths and the needs of our entities to be able to help refugees, prioritizing those that can be covered by one of the partners through the exchange of good practices.</p>",2019,"Erasmus+, Young Refugees, Self-employement, Exchange of good practices",10.5281/zenodo.2616791,,publication
"CIMCB/MetabWorkflowTutorial: Jupyter Notebooks for Metabolomics Tutorial (Metabolomics, Mendez et.al. 2019)","Leighton Pritchard, Kevin Mendez","README.md - Supplementary Information for <code>Mendez et.al. ""Toward Collaborative Open Data Science in Metabolomics using Jupyter Notebooks and Cloud Computing"" Metabolomics, 2019.</code>
&lt;p align=""justify""&gt;
This repository contains the supplementary information for the ""Toward Collaborative Open Data Science in Metabolomics using Jupyter Notebooks and Cloud Computing"" tutorial review. The focuses is on experiential learning using an example interactive metabolomics data analysis workflow deployed using a combination of Python, Jupyter Notebooks and Binder. The three-step pedagogical process of understanding, implementation, and deployment is broken down into 5 tutorials.
&lt;/p&gt;&lt;p align=""justify""&gt;
The tutorials takes you through the process of using interactive notebooks to produce a shareable, reproducible data analysis workflow that connects the study design to reported biological conclusions in an interactive document. The workflow implemented includes a discrete set of interactive and interlinked procedures: data cleaning, univariate statistics, multivariate machine learning, feature selection, and data visualisation. 
&lt;/p&gt;<p>&lt;br /&gt;</p>
Quick Start
<em>To launch the tutorial environment in the cloud:</em> <a href=""https://mybinder.org/v2/gh/cimcb/MetabWorkflowTutorial/master""></a>
Tutorial 1:
<ul>
<li><a href=""https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html"">Tutorial 1 static notebook</a></li>
<li><a href=""https://mybinder.org/v2/gh/CIMCB/MetabWorkflowTutorial/master?filepath=Tutorial1.ipynb""></a></li>
</ul>
Tutorial 2:
<ul>
<li><a href=""https://cimcb.github.io/MetabWorkflowTutorial/Tutorial2.html"">Tutorial 2 static notebook</a></li>
<li><a href=""https://mybinder.org/v2/gh/CIMCB/MetabWorkflowTutorial/master?filepath=Tutorial2.ipynb""></a></li>
</ul>
Tutorial 4:
<ul>
<li><a href=""https://cimcb.github.io/MetabSimpleQcViz/Tutorial4.html"">Tutorial 4 static notebook</a></li>
<li><a href=""https://mybinder.org/v2/gh/cimcb/MetabSimpleQcViz/master?filepath=Tutorial4.ipynb""></a></li>
</ul>
<p>&lt;br /&gt;</p>
Tutorials
<ol>
<li><a href=""#one"">Launching and using a Jupyter notebook on Binder</a></li>
<li><a href=""#two"">Interacting with and editing a Jupyter notebook on Binder</a></li>
<li><a href=""#three"">Downloading and installing a Jupyter notebook on a local machine</a></li>
<li><a href=""#four"">Creating a new Jupyter notebook on a local computer</a></li>
<li><a href=""#five"">Deploying a Jupyter notebook on Binder via GitHub</a></li>
</ol>
<p>&lt;br /&gt;</p>
<p>&lt;a id=""one""&gt;&lt;/a&gt;</p>
Tutorial 1: Launching and using a Jupyter notebook on Binder
&lt;p align=""justify""&gt;
&lt;i&gt;In this tutorial we will step though a metabolomics computational workflow that has already been implemented as a Jupyter Notebook and deployed on Binder. In this workflow we will interrogate a published  &lt;a href=""https://www.nature.com/articles/bjc2015414""&gt;(Chan et al., 2016)&lt;/a&gt; NMR urine data set (deconvolved and annotated) used to discriminate between samples from gastric cancer and healthy patients. The data set is available in the &lt;a href=""http://www.metabolomicsworkbench.org""&gt;Metabolomics Workbench data repository&lt;/a&gt; (Project ID PR000699). The data can be accessed directly via its project &lt;a href=""http://dx.doi.org/DOI:10.21228/M8B10B""&gt;DOI:10.21228/M8B10B&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;&lt;p align=""justify""&gt; &lt;i&gt;Prior to beginning this tutorial, it is recommended to view the provided static version of the &lt;code&gt;Jupyter&lt;/code&gt; analysis notebook: &lt;a href=""https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html""&gt;Tutorial 1: Static Notebook&lt;/a&gt;&lt;/i&gt;
&lt;/p&gt;Tutorial 1 steps:
<ol>
<li>Launch Binder by clicking the ""Launch Binder"" icon: <a href=""https://mybinder.org/v2/gh/cimcb/MetabWorkflowTutorial/master""></a></li>
<li>Click on the Tutorial1.ipynb filename to open the Jupyter Notebook</li>
<li>Run the code cells:<br>
 3.1. Click anywhere within the cell, which will then be outlined by a green box (or blue for text cells)<br>
 3.2. Click on the ""Run"" button in the top menu to execute the code in the cell    </li>
<li><strong>Alternatively</strong>, run multiple cells:<br>
 4.1. In the ""Cell"" menu item, choose ""Run All"" (runs all cells in the notebook, from top to bottom)<br>
 4.2. In the ""Cell"" menu item, choose ""Run all below"" (runs all cells below the current selection) </li>
<li>Download notebook (as changes to the notebook are lost when the Binder session end):<br>
 5.1. Return to Jupyter landing page, by choosing ""File"" then ""Open..""<br>
 5.2. Click the checkbox next to each file you wish to download<br>
 5.3. Click the 'Download' button from the top menu  </li>
</ol>
<p>&lt;br /&gt;</p>
<p>&lt;a id=""two""&gt;&lt;/a&gt;</p>
Tutorial 2: Interacting with and editing a Jupyter notebook on Binder
&lt;p align=""justify""&gt;&lt;i&gt;
The fuctionality of the notebook in Tutorial 2 is identical to Tutorial 1, but now the text cells have been expanded into a comprehensive interactive tutorial. Text cells with a yellow background provide the metabolomics context and describe the purpose of the code in the following code cell. Additional coloured text boxes are placed throughout the workflow to help novice users navigate and understand the interactive principles of a Jupyter Notebook:
&lt;/i&gt;&lt;/p&gt;

Box
Description
Purpose




Action
red background labelled with 'gears' icon
Suggestions for changing the functionality of the subsequent code cell by editing (or substituting) a line of code


Interaction
green background with 'mouse' icon
Suggestions for interacting with the visual results generated by a code cell


Notes
blue background with 'lightbulb' icon
Further information about the theoretical reasoning behind the block of code or a given visualisation



&lt;p align=""justify""&gt;
&lt;i&gt;There is a second data included with this  tutorial which has been converted to our standardised &lt;a href=""https://en.wikipedia.org/wiki/Tidy_data""&gt;Tidy Data&lt;/a&gt; This data has been previously published as an article &lt;a href=""https://link.springer.com/article/10.1007%2Fs11306-016-1059-9""&gt;Gardlo et al. (2016)&lt;/a&gt;, in  Metabolomics. Urine samples collected from newborns with perinatal asphyxia were analysed, and the deconvoluted and annotated file is deposited at the &lt;a href=""https://www.ebi.ac.uk/metabolights/""&gt;Metabolights&lt;/a&gt; data repository (Project ID &lt;a href=""https://www.ebi.ac.uk/metabolights/MTBLS290""&gt;MTBLS290&lt;/a&gt;). &lt;/i&gt;
&lt;/p&gt;&lt;p align=""justify""&gt;
&lt;i&gt;Prior to beginning this tutorial, it is recommended to view the provided static version of the &lt;code&gt;Jupyter&lt;/code&gt; analysis notebook: &lt;a href=""https://cimcb.github.io/MetabWorkflowTutorial/Tutorial2.html""&gt;Tutorial 2: Static Notebook&lt;/a&gt;&lt;/i&gt;
&lt;/p&gt;Tutorial 2 steps:
<ol>
<li>Launch Binder by clicking the ""Launch Binder"" icon: <a href=""https://mybinder.org/v2/gh/cimcb/MetabWorkflowTutorial/master""></a></li>
<li>Click on the Tutorial2.ipynb filename to open the Jupyter Notebook</li>
<li>Run the code cells:<br>
 3.1. Click anywhere within the cell, which will then be outlined by a green box (or blue for text cells)<br>
 3.2. Click on the ""Run"" button in the top menu to execute the code in the cell </li>
<li><strong>Modify and run code cells</strong>:<br>
 4.1. When prompted, complete one (or multiple) modifications suggested in each 'action' box .<br>
 4.2. Click ""Run all below"" from the ""Cell"" dropdown menu, observing the changes in cell output for all the subsequent  cells. </li>
<li>Download notebook (as changes to the notebook are lost when the Binder session end):<br>
 5.1. Return to Jupyter landing page, by choosing ""File"" then ""Open..""<br>
 5.2. Click the checkbox next to each file you wish to download<br>
 5.3. Click the 'Download' button from the top menu  </li>
</ol>
<p><strong>Note: Further guidance for step 4 is included in the notebook itself.</strong></p>
<p>&lt;br /&gt;</p>
<p>&lt;a id=""three""&gt;&lt;/a&gt;</p>
Tutorial 3: Downloading and installing a Jupyter notebook on a local machine.
&lt;p align=""justify""&gt;
&lt;i&gt;For Tutorial 3, we will provide the process of installing Python and Jupyter, and reproducing Tutorial 1 on your own machine. The Anaconda distribution provides a unified, platform-independent framework for running notebooks and managing Conda virtual environments that is consistent across multiple operating systems, so for convenience we will use the Anaconda interface in these tutorials.&lt;/i&gt;
&lt;/p&gt;Tutorial 3 steps:
Part A) Install Jupyter and Python using Anaconda
<ol>
<li>Go to the <a href=""https://www.anaconda.com/distribution/"">Official Anaconda Website</a> and click the 'Download' button. </li>
<li>Press the 'Download' button under the 'Python 3.x version' in Bold to download the graphical installer for your OS. </li>
<li>After the download has finished, open (double-click) the installer to begin installing the Anaconda Distribution</li>
<li>Follow the prompts on the graphical installer to completely install the Anaconda Distribution</li>
</ol>
Part B) Create a virtual Environment, and start Tutorial 1.
<p><strong>Note: If you are using Windows, you need to install git using the following: <a href=""https://gitforwindows.org/"">Git for Windows</a></strong></p>
<ol>
<li>Open Terminal on Linux/MacOS or Command Prompt on Windows</li>
<li>Enter the following into the console (one line at a time)</li>
</ol>
<pre><code class=""lang-console"">git clone https://github.com/cimcb/MetabWorkflowTutorial
cd MetabWorkflowTutorial
conda env create -f environment.yml
conda activate MetabWorkflowTutorial
jupyter notebook
</code></pre>
<p>&lt;br /&gt;</p>
<p>&lt;a id=""four""&gt;&lt;/a&gt;</p>
Tutorial 4: Creating a new Jupyter notebook on a local computer
&lt;p align=""justify""&gt;
&lt;i&gt;The Jupyter notebook we will generate for this data set demonstrates the use of visualisation methods available in Anaconda Python without the need to install additional 3rd party packages. This tutorial is made available in the GitHub repository at https://github.com/cimcb/MetabSimpleQcViz. In the notebook, we will load the data set and produce four graphical outputs:&lt;/i&gt;
&lt;/p&gt;<ol>
<li>A histogram of the distribution of QCRSD across the data set.</li>
<li>A kernel density plot of QCRSD vs. D-ratio across the data set.</li>
<li>A PCA scores plot of the data set labelled by sample type.</li>
<li>A bubble scatter plot of molecular mass vs. retention time, with bubble size proportional to QCRSD</li>
</ol>
<p><em>Prior to beginning this tutorial, it is recommended to view the provided static version of the <code>Jupyter</code> analysis notebook: <a href=""https://cimcb.github.io/MetabSimpleQcViz/Tutorial4.html"">Tutorial 4: Static Notebook</a></em></p>
Tutorial 4 steps:
<ol>
<li>Download and unzip the <a href=""https://github.com/cimcb/MetabSimpleQcViz"">repository</a> to a folder on your own computer, as in tutorial 3.</li>
<li>Create an new notebook:<br>
 2.1. Ensure ""[base (root)]"" is selected in the ""Applications on"" dropdown list of the main panel<br>
 2.2. Launch Jupyter Notebook<br>
 2.3. Navigate to the repository root (the ""MetabSimpleQcViz"" folder)<br>
 2.4. Click on the ""New"" button in the top right corner of the page<br>
 2.5. Select ""Python 3"" from the list of supported languages (a black Jupyter Notebook called ""Untitled"" will open)<br>
 2.6. Rename the notebook by clicking on the text ""Untitled"" and replacing it with ""myQCviz""     </li>
<li>Edit the notebook (using the template file as a guide) ...   </li>
<li>Save and close the notebook:<br>
 4.1. Save by clicking on the floppy disk icon (far left on the menu)<br>
 4.2. Close by clicking ""File"" and then ""Close and Halt""from the top Jupyter menu<br>
 4.3. The Jupyter session can be closed by clicking on ""Quit"" on the Jupyter landing page tab of your web browser .   </li>
</ol>
<p>&lt;br /&gt;</p>
<p>&lt;a id=""five""&gt;&lt;/a&gt;</p>
Tutorial 5: Deploying a Jupyter notebook on Binder via GitHub
<ol>
<li>Create a GitHub account (please follow the instructions on GitHub at <a href=""https://help.github.com/en/articles/signing-up-for-a-new-github-account"">https://help.github.com/en/articles/signing-up-for-a-new-github-account</a>) </li>
<li>Create a new repository<br>
 2.1. Click on the ""Repositories"" link at the top of the page<br>
 2.2. Enter ""Jupyter_metabQC"" into the ""Repository name"" field 
 2.3. Select the 'Public' option for the repository<br>
 2.3. Select the checkbox to ""Initialize this repository with a ""README""<br>
 2.4. Add a license (<a href=""https://choosealicense.com/"">https://choosealicense.com/</a>)<br>
 2.5. Click the ""Create repository"" button </li>
<li>Add files (new Jupyter notebook and the Excel data file ) to the repository<br>
 3.1. Click on the 'Upload files' button<br>
 3.2. Drag the files (myQCviz.ipynb' and 'data.xlsx') from your computer<br>
 3.3. Enter the text ""Add data and notebook via upload"" to the top field under ""Commit changes.""<br>
 3.4. Commit the files to the repository, click on the ""Commit changes"" button </li>
<li>Build and launch a Binder virtual machine for this repository<br>
 4.1. Open <a href=""https://mybinder.org"">https://mybinder.org</a> in a modern web browser<br>
 4.2. Enter the path to the home page of your repository (<a href=""https://github.com/account_name/Jupyter_metabQC"">https://github.com/account_name/Jupyter_metabQC</a>)<br>
 4.3. Click the 'Launch' button 
 4.4. The URL shown in the field ""Copy the URL below and share your Binder with others"" can be shared with colleagues   </li>
</ol>",2019,,10.5281/zenodo.3362625,,software
Securing Cloud Computing Through IT Governance,"Salman M. Faizi, Shawon Rahman,","<p>Abstract</p>

<p>Lack of alignment between information technology (IT) and the business is a problem facing many organizations. Most organizations, today, fundamentally depend on IT. When IT and the business are aligned in an organization, IT delivers what the business needs and the business is able to deliver what the market needs. IT has become a strategic function for most organizations, and it is imperative that IT and business are aligned. IT governance is one of the most powerful ways to achieve IT to business alignment. Furthermore, as the use of cloud computing for delivering IT functions becomes pervasive, organizations<br>
using cloud computing must effectively apply IT governance to it. While cloud computing presents tremendous opportunities, it comes with risks as well. Information security is one of the top risks in cloud computing. Thus, IT governance must be applied to cloud computing information security to help manage the risks associated with cloud computing information security. This study advances knowledge by extending IT governance to cloud computing and information security governance.</p>

<p>Keywords</p>

<p>Business alignment, cloud computing, cloud computing security, information security, IT governance.</p>",2019,"IT governance, information security,, cloud computing security, cloud computing,, Business alignment,",10.5281/zenodo.2572574,,publication
THE ERA OF CLOUD COMPUTING: A RECENT SURVEY.,"Apurva Saxena, Pratima gautam.","<p>Cloud computing an emerging technology provides various services to the users like infrastructure, hardware, software, storage etc. For working cloud in data security, it is necessary that cloud computing network should always be free from outside attack/ threats. Here in this paper we focused on the cloud computing era needs, applications, issues and challenges and also tried to propose some of the good solutions which can be implemented to resolve them. Authors also tried to discuss different types of cloud and their providers with current scenario of cloud computing which pave the way to forecast its basic way to future. The service of cloud like IaaS (Infrastructure as a Service) allows an internet trade a way to build up and produce on demand.PaaS (Platform as a Service): It is the client who controls the applications that run in the environment, but does not manage the operating system, hardware on which they are running. Need of cloudcomputing is an expertise uses the internet and central remote servers to maintain data and applications. Some reasons are Speedy Elasticity, Measured Service, on-Demand Self-Service etc. Cloud computing is also known as fifth generation of computing after supercomputer, Personal Computer, Client-Server Computing, and the Web. One of the issues of cloud computing is data security in which data can be loss or hacked by the attacker, possible solution of this problem by applying encryption techniques on the data. Some challenges are also present in cloud computing like lack of resources in which staff needed new skills or updated knowledge related technology, possible solution is to recruit new staff or give training to the existing ones. This survey of cloud computing concluded that Cloud computing is increasing part of IT and many gigantic organizations are going to implement cloud computing. In the near future work on data science, artificial intelligence and machine learning service inside cloud provider to protect the customer sensitive data from the intruders attack.</p>",2019,Cloud computing virtual machine attack issues challenge.,10.5281/zenodo.3269464,,publication
A SURVEY ON PRIVACY PRESERVING HEALTHCARE DATA USING CLOUD COMPUTING,"M. Rajakumar, Dr. S. Thavamani","<p>Cloud Computing is a new way of delivering computing resources and services. It is a model for on-demand network access to a shared pool of configurable computing resources like, networks, servers, storage, applications, and services that can be provided with minimal management effort or service provider interaction. Healthcare is faster growing way to adopt cloud computing. It is very important for every individual and essential for every country in the Globe. Electronic healthcare systems in the world are moving towards a more accessible, collaborative and more proactive way in reaching out to the public. The delivering of public health solutions can lead to increased efficiencies in health related data. Many nations across the globe have launched aggressive stimulus programs aimed at solving public healthcare problems in efficient way. This review article mainly focus on different ICT based infrastructure facilities available in various hospitals in India, abroad with cost effective manner using cloud computing technologies, services and this will be a best solutions for healthcare systems in rural areas. And also in this paper presented about various cloud service providers, investment in healthcare, <strong>IT adoption in Indian Healthcare sector, </strong>Major benefits of Cloud-based Patient Management System [PMS], about SADA systems, Top ten cloud storage companies in healthcare and Pros and Cons of EHR systems, comparison of Indian healthcare systems with US system, and various Cloud Simulators.</p>",2019,"Electronic Healthcare Systems, ICT Based Infrastructure, EHR systems, Indian Healthcare Sector & Cloud Simulators.",10.5281/zenodo.2924583,,publication
Survey of Android Apps for Agriculture Sector,"Hetal Patel, Dharmendra Patel","<p>India is an agriculture based developing country. Information dissemination to the knowledge intensive agriculture sector is upgraded by mobile-enabled information services and rapid growth of mobile telephony. It bridge the gap between the availability of agricultural input and delivery of agricultural outputs and agriculture infrastructure. Mobile computing, cloud computing, machine learning and soft computing are the immerging techniques which are being used in almost all fields of research. Apart from this, they are also useful in our day-to-day activities such as education, medical and agriculture. This paper explores how Android Apps of agricultural services have impacted the farmers in their farming activities.</p>",2019,"Android Apps, Mobile computing, Machine learning, Cloud computing",10.5121/ijist.2016.6207,,publication
AN OPTIMIZED AND ARTIFICIAL INTELLIGENCE BASED TASK SCHEDULING IN PARALLEL CLOUD COMPUTING,Hema Yadav,"<p>In parallel computing, scheduling can be defined as a collection of laws in which execution order has been defined in detailed manner. To perform scheduling in parallel computing, multiprocessor is utilized and it required to arranging the resources for all processors. The problem of task scheduling can be utilized to finding or, searching the best allocation of distinguished task from available resources such as processors, computers to obtain desired aim. Task scheduling and managing the resources is the main challenge of parallel computing to resolve this issue, n this proposed work the DAG (Directed Acyclic graph) is used. To optimize the resources and scheduling of task better here the optimization algorithm is utilized named as Cuckoo Search (CS). After get the optimized task for classification purpose the neural network i.e. Artificial Neural Network (ANN) is applied here, by which it&rsquo;s easy to allocation of resources to processes.</p>",2019,,10.5281/zenodo.3544777,,publication
Airflow Directed Acyclic Graph,"Shubha B G, A.M.Prasad","<p><em>The paper &ldquo;Airflow Dags&rdquo; is based on the generation of directed acyclic graphs during the creation of profiles and roles in the Enterprise management software. Profiles are facilities that are provided to the clients in organization, they are seen in the configuration view of enterprise software. Roles are the permissions and the access that is given to a client. Automation is necessary to make repetitive and difficult procedures simpler. In this case the automation is done so that the onboarding of new clients in organization is done in an easier way. Previously this was done manually which was time consuming, the development of this project has made the process simpler. It parses the data from the pod directory json file to generate client list, pod configurations. These are then stored in a config which is made for each of the pods. The details are then taken from this repository as the details cannot be hard coded into the code. This makes the program seem more discrete.The project work recorded below shows the implementation using an open source software called airflow which is used to generate directed acyclic graph.</em></p>",2019,"Automation, airflow, enterprise management software, profiles, roles",10.5281/zenodo.3247274,,publication
THE REALIZATION OPPORTUNITY OF IDEAL ENERGY SYSTEM USING NANOTECHNOLOGY BASED RESEARCH AND INNOVATIONS,"Shubhrajyotsna Aithal, P. S. Aithal","<p>An energy system is primarily designed to produce or convert and deliver energy for useful work. It supports the dynamic functions of the people both for their basic needs and luxurious wants. Out of many energy sources used in practice, renewable energy sources are finding importance due to their inherent ability to support a sustainable world. The challenges of developing such an efficient system can be handled effectively by considering the model and the characteristics of the ideal energy system. In our previous paper, we have developed a model and identified about 34 characteristics of an ideal energy system as a predictive hypothetical system and discussed the possibility of developing at least optimum energy system using suitable technology. In this paper, we made an attempt to use nanotechnology, one of the two universal technologies of the 21<sup>st</sup> century to realize many characteristics of an ideal energy system. We also proposed and analysed the possibility of using some nonlinear Dye Sensitized Nanocomposite doped Polymer Films in the process of designing highly efficient, low cost solar energy to electric energy converters.This predictive analysis opens up various research possibilities of nanomaterials usage in developing optimum energy systems towards the objective of achieving ideal energy systems.</p>",2019,"Energy, System, Ideal Energy System, Renewable Energy Systems & Nanotechnology",10.5281/zenodo.2531876,,publication
