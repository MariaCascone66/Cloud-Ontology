title,authors,abstract,year,keywords,doi,url,type
Model of cloud-based services for data mining analysis,"Aleksandar Karadimce, Slobodan Kalajdziski, Danco Davcev","<p>New cloud-based services are being developed constantly in order to meet the need for faster, reliable and scalable methods for knowledge discovery. The major benefit of the cloud-based services is the efficient execution of heavy computation algorithms in the cloud simply by using Big Data storage and processing platforms. Therefore, we have proposed a model that provides data mining techniques as cloud-based services that are available to users on their demand. The widely known data mining algorithms have been implemented as Map/Reduce jobs that are been executed as services in cloud architecture. The user simply chooses or uploads the dataset to the cloud, makes appropriate settings for the data mining algorithm, executes the job request to be processed and receives the results. The major benefit of this model of cloud-based services is the efficient execution of heavy computation data mining algorithm in the cloud simply by using the Ankus - Open Source Big Data Mining Tool and StarfishHadoop Log Analyzer. The expected outcome of this research is to offer the integration of the cloud-based services for data mining analysis in order to provide researchers with reliable collaborative data mining analysis model.</p>",2016,"data mining, cloud computing services, Map/Reduce, web services, knowledge discovery",10.5539/cis.v8n4p40,,publication
Software Educativo para o ensino de vetores integrado aos conceitos de Cloud Computing e M-Learning,"Patricia Mariotto Mozzaquatro, Leo Natan Paschoal, Michele Ferraz Figueiró, Fabricio Soares Kronbauer, Rodrigo Luiz Antoniazzi","<p>It&acute;s been living a moment of big transformations and technological advances in the educational context. There is a variety of information searches, technologies and manners of available communication. The advances of Information and Communication Technologies is changing the way of how the user faces the technology, making available intelligent interfaces, multimedia resources, wireless communication and high velocities in the access to web data. Technologies, mainly the new technological environmental , in this case, the mobile devices can be considered as a tool in the measure of knowledge processes. In this context, this research seeked to develop an educational software to help in the solution of exercises about vectors, in order to use the mobile computing in the processes of teaching &ndash; learning. The software was validated by applying the tests of white box and black box. It is evident a good reception from the public.</p>",2016,,10.5281/zenodo.59426,,publication
Business Process as a Service (BPaaS): A Model-Based Approach for Smart Business and IT-Cloud Alignment,"Stefan Wesner, Jörg Domaschka, Robert Woitsch, Wilfrid Utz","<p>The use of cloud computing for the benefit of business is an ambitious goal considering the gap between domain-oriented business processes and executable workflows within a Cloud environment. This tutorial teaches a model based approach, where (a) business process specify the domain, (b) workflow models are used to orchestrate cloud offerings, (c) decision models are used for cloud infrastructure adaptations and (d) ontologies are used to semantically glue all parts together. Such a model-based approach enables both, (i) human oriented knowledge technologies and (ii) machine oriented knowledge technologies for a hybrid knowledge processing. This tutorial targets the practical use, the adaptation and implementation of aforementioned model-based knowledge processing and hence targets cloud brokers. These are limited by current “off the shelf” solutions that focus on delivering virtual server, but do not offer the possibility to “re-implement” individualized solution from scratch on a more business oriented level. Hence this tutorial provides initial solutions use but focuses on the adaptation, extension and re-implementation features of the enabling meta-modelling platform. The audience gets introduced into one of the leading meta-modelling platforms called ADOxx, in form of the world-wide active adoxx.org community, which provides tutorials and training in different application domains.</p>",2016,,10.5281/zenodo.164149,,publication
Towards Knowledge-Based Assisted IaaS Selection,"Kyriakos Kritikos, Kostas Magoutis, Dimitris Plexousakis","<p>Current PaaS platforms enable single or hybrid cloud deployments. However, such deployment types cannot best cover the user application requirements as they do not consider the great variety of services offered by different cloud providers and the effects of vendor lock-in. On the other hand, multi-cloud deployment enables selecting the best possible service among equivalent ones providing the best trade-off between performance and cost. In addition, it avoids cases of service level deterioration due to service underperformance as main effects of vendor lock-in. While many multi-cloud application deployment research prototypes have been proposed, such prototypes do not examine the effect that deployment decisions have on application performance. As such, they blindly attempt to satisfy low-level hardware requirements by neglecting the impact of allocation decisions on higher-level requirements at the component or application level. To this end, this paper proposes a new IaaS selection algorithm which, apart from being able to satisfy both low and high level requirements of different types, it also exploits deployment knowledge offered via reasoning over previous application execution histories to take the best possible allocation decisions. The experimental evaluation clearly shows that by considering this extra knowledge, more optimal deployment solutions are derived, able to maintain the service levels requested by users, in less solving time.</p>",2016,"IaaS, selection, knowledge base, rules, evaluation, requirements, placement, location, security, performance, quality of service, deployment",10.5281/zenodo.164165,,publication
D3.1_Procurement Barriers Report,"Marina Bregu, Damir Savanovic, Daniele Catteddu","<p>The acquisition of IT services is key to any public or private organisation and the advent of cloud computing requires innovation in the procurement of cloud services.</p>

<p>Although cloud computing has become increasingly popular, it appears that potential customers, in the public sector in general and in the research community in particular, are facing barriers that inhibit the wider adoption of cloud services.</p>

<p>This report presents a list of barriers to cloud services procurement identified through literature, in-depth interviews of IT managers surveyed over a period of 3 months, and input provided by the PICSE Task Force members as well as intergovernmental research organisations such as CERN and EMBL. The survey demonstrated that barriers to cloud service procurement mainly relate to the adoption of new technology (i.e. cloud computing) and the procurement process itself.</p>

<p>The first category encompasses legal jurisdiction impediments, which Eurostat [1] highlighted as one of the main barriers to the procurement of cloud services. Indeed, services are often hosted in one country and consumed in another, hence cloud consumers&rsquo; uncertainty about data location and the applicable laws in case of dispute or in relation to compliance. Prerequisites such as expertise and knowledge of both contractual and operational aspects also impede the purchase of cloud computing services. The barriers posed by the procurement process vary depending on the type of process used. Restricted procurement processes suffer from a lack of competition and higher costs while open procurement processes are time consuming, require detailed specifications to be ready at the start of the procurement process and therefore lead to higher tendering and evaluation costs.</p>

<p>Cloud marketplaces and brokerage services are seen as aiding the procurement process but suffering from under-investment and thus not sufficiently mature. In addition, the nature of cloud services and the pay-per-use method can complicate budget planning for research organisations. Therefore, framework procurement agreements are perceived as a good alternative for the procurement of cloud services; along with PCP (Pre-Commercial Procurement), PPI (Public Procurement of Innovative solutions), and JPA (Joint Procurement Actions), which could potentially fulfil the needs of the research community.</p>

<p>To combat this situation and increase the uptake of cloud computing in the public research sector, cloud service providers (CSPs) are advised to increase transparency in their offers; particularly regarding security, privacy and data management, ensure the trustworthiness of their privacy policies and improve the Service Level Agreements (SLA) so that their offers stand out.</p>

<p>Similarly users are advised to acknowledge that the commoditisation of IT services that cloud services represents has economic advantages and that customisation of those services will increase procurement costs and potentially increase service provider dependence.</p>",2016,"Procurement, barriers, cloud services",10.5281/zenodo.18309,,publication
"Statika: managing cloud resources, bioinformatics tools and data","Alekhin, Alexey","<p>Next Generation Sequencing (NGS) has brought a revolution to the bioinformatics landscape, definitely reshaping fields such as genomics and transcriptomics, by offering sheer amounts of data about previously inaccessible domains in a cheap and scalable way. Thus biological data analysis demands, more than ever, high performance computing architectures; in particular, Cloud Computing, a comparable breakthrough in the IT world, holds promise for being the foundation on which a solution could be built (as already demonstrated by pioneering efforts such as Galaxy or CloudBioLinux). It provides a perfect framework for high throughput data analysis: deploying architectures with as much computing capacity as needed, scaling in an horizontal way, being also able to scale down adjusting to the computing needs real time, or the pay-as-you-go model make for a strong case.</p>

<p>However, fast, reproducible, and cost-effective data analysis in the cloud at such scale remains elusive. Certainly, one fundamental prerequisite for achieving this is having the ability to manage both the tools and data to be used in a robust, reproducible, and automated way. High throughput analysis, where a lot of resources are to be used and paid for, needs to have a robust configuration system to rely on. In the cloud computing world, due to its on-demand nature, automated resource configuration is a critical factor. This is even more so in the case of bioinformatics analysis where pretty often a pretty intricated and unstable chain of dependencies underlies tools and data; knowing beforehand that all the resources to be used are properly configured is invaluable.</p>

<p>Statika (http://ohnosequences.com/statika) aims to be a basic tool for the declaration and deployment of composable, versioned and reproducible cloud infrastructures for the bioinformatics space.</p>

<p>Data, tools and infrastructure are treated on an equal footing, and a expressive domain specific language allows the user to express complex dependency relationships, check for possible version conflicts and automatically choose a safe resource creation order.&nbsp;</p>

<p>By making use of advanced features of the Scala programming language such as dependent types and type-level computations a great deal of structure can be expressed abstractly, and checked at compile time before any cost is incurred. A strong versioning system where both data and tools are included makes reproducibility not only possible but actually enforced.&nbsp;</p>

<p>Statika has been put to work on scenarios as different as a cloud-based system for scaling inherently parallel computations in the bioinformatics domain: Nispero, or by providing versioned and modular automated deployments of Bio4j, a graph database integrating all data from key resources in the bioinformatics data space, including: UniProt, Gene Ontology, the NCBI Taxonomy or UniRef. We use it internally for the integration and automated deployment of all sort of bioinformatics tools and data.</p>

<p>Statika is open source, available under the AGPLv3 license.</p>

<p>&nbsp;</p>

<div>&nbsp;</div>",2016,,10.5281/zenodo.35101,,poster
Statika: managing bioinformatics tools and resources in the cloud,"Alekhin, Alexey","<p>Next Generation Sequencing has revolutionized the bioinformatics landscape, reshaping fields such as genomics and transcriptomics, by offering huge amounts of data about previously inaccessible domains in a cheap and scalable way. Thus, biological data analysis demands, more than ever, high performance computing architectures. Cloud Computing, a comparable breakthrough in the IT world, holds promise for being the foundation on which a solution could be built (as already demonstrated by pioneering efforts such as Galaxy or CloudBioLinux). It provides a perfect framework for high throughput data analysis: deploying architectures with as much computing capacity as needed, scaling in a horizontal way, being also able to scale down adjusting to the computing needs real time, with the pay-as-you-go model.</p>

<p>However, fast and cost-effective data analysis in the cloud at such scale remains elusive. High throughput analysis, where a lot of resources are to be used and paid for, critically needs to have an ability to manage both the tools and data in a robust, reproducible and automated way. As in bioinformatics analysis often a pretty complex and unstable chain of dependencies underlies tools and data, knowing beforehand that all the resources to be used are properly configured is invaluable.</p>

<p>Statika (http://ohnosequences.com/statika) aims to be a basic tool for the declaration and automated deployment of composable cloud infrastructures for the bioinformatics space. Using Statika data, tools and infrastructure are treated on an equal basis with a expressive domain specific language that allows the user to express complex dependency relationships. Statika will automatically check for possible version conflicts and choose a safe resource creation order.</p>

<p>Statika has been applied in different scenarios: from a cloud-based system for scalable and composable parallel computations in the bioinformatics domain as in Nispero tool, to modular automated deployments of complex databases as Bio4j. Bio4j (bio4j.com)is a graph database integrating all data from key resources in the bioinformatics data space, including UniProt, Gene Ontology, the NCBI Taxonomy or UniRef. We use Statika internally for the integration and automated deployment of all sort of bioinformatics tools and data.</p>

<p>Statika is open source, available under the AGPLv3 license.</p>

<p>&nbsp;</p>

<p>&nbsp;</p>

<div>&nbsp;</div>",2016,,10.5281/zenodo.35104,,poster
Preventing Information Inference in Access Control,"Paci Federica , Zannone Nicola ","<p>Technological innovations like social networks, personal devices<br />
and cloud computing, allow users to share and store online a huge<br />
amount of personal data. Sharing personal data online raises significant<br />
privacy concerns for users, who feel that they do not have<br />
full control over their data. A solution often proposed to alleviate<br />
users&rsquo; privacy concerns is to let them specify access control<br />
policies that reflect their privacy constraints. However, existing approaches<br />
to access control often produce policies which either are<br />
too restrictive or allow the leakage of sensitive information. In this<br />
paper, we present a novel access control model that reduces the risk<br />
of information leakage. The model relies on a data model which<br />
encodes the domain knowledge along with the semantic relations<br />
between data. We illustrate how the access control model and the<br />
reasoning over the data model can be automatically translated in<br />
XACML.We evaluate and compare our model with existing access<br />
control models with respect to its effectiveness in preventing leakage<br />
of sensitive information and efficiency in authoring policies.<br />
The evaluation shows that the proposed model allows the definition<br />
of effective access control policies that mitigate the risks of<br />
inference of sensitive data while reducing users&rsquo; effort in policy<br />
authoring compared to existing models.</p>",2016,"XACML, personal data, Access controls, complexity measures, Information Systems",10.1145/2752952.2752971,,publication
Metapasta,"Evdokim Kovach, Eduardo Pareja Tobes, Raquel Tobes, Marina Manrique","<p>Metapasta is an open-source, fast and horizontally scalable tool for community profiling based on the analysis of 16S metagenomics data. It is entirely cloud-based and specifically designed to take advantage of it: it performs the community profiling of a sample starting from raw Illumina reads in approximately 1 hour, needing approximately the same time for doing the same on hundreds of samples. It uses BLAST or LAST, but other mapping solutions can be integrated. The taxonomic assignment is done using a best hit and a lowest common ancestor paradigm taking the NCBI taxonomy as reference. As an output, Metapasta generates the frequencies of all the identified taxa in any of the samples in tab-separated value text files. This output includes direct assignment frequencies and cumulative frequencies based on the hierarchical structure of the taxonomy tree. Reports format can be configured using DSL similar to spreadsheet formulas. PDF files with assigned taxonomy tree can be rendered.</p>

<p>Metapasta is implemented in Scala and based on cloud computing (Amazon Web Services). The graph data platform&nbsp;Bio4jis used for retrieving taxonomy related information and the tool&nbsp;Compota&nbsp;is used for distributing and coordinating compute tasks.</p>",2016,"metagenomics, 16S, AWS",10.5281/zenodo.35367,,software
Efforts towards accessible and reliable bioinformatics,"Kalaš, Matúš","<p>The aim of the presented work was contributing to making scientific computing more accessible, reliable, and thus more efficient for researchers, primarily computational biologists and molecular biologists. Many approaches are possible and necessary towards these goals, and many layers need to be tackled, in collaborative community efforts with well-defined scope. As diverse components are necessary for the accessible and reliable bioinformatics scenario, our work focussed in particular on the following:</p>

<p>In the BioXSD project, we aimed at developing an XML-Schema-based data format compatible with Web services and programmatic libraries, that is expressive enough to be usable as a common, canonical data model that serves tools, libraries, and users with convenient data interoperability.</p>

<p>The EDAM ontology aimed at enumerating and organising concepts within bioinformatics, including operations and types of data. EDAM can be helpful in documenting and categorising bioinformatics resources using a standard &ldquo;vocabulary&rdquo;, enabling users to find respective resources and choose the right tools.</p>

<p>The eSysbio project explored ways of developing a workbench for collaborative data analysis, accessible in various ways for users with various tasks and expertise. We aimed at utilising the World-Wide-Web and industrial standards, in order to increase compatibility and maintainability, and foster shared effort.</p>

<p>In addition to these three main contributions that I have been involved in, I present a comprehensive but non-exhaustive research into the various previous and contemporary efforts and approaches to the broad topic of integrative bioinformatics, in particular with respect to bioinformatics software and services. I also mention some closely related efforts that I have been involved in.</p>

<p>The thesis is organised as follows: In the <em>Background</em> chapter, the field is presented, with various approaches and existing efforts. <em>Summary of results</em> summarises the contributions of my enclosed projects &ndash; the BioXSD data format, the EDAM ontology, and the eSysbio workbench prototype &ndash; to the broad topics of the thesis. The <em>Discussion</em> chapter presents further considerations and current work, and concludes the discussed contributions with alternative and future perspectives.</p>

<p>In the printed version, the three articles that are part of this thesis, are attached after the <em>Discussion</em> and References. In the electronic version (in PDF), the main thesis is optimised for reading on a&nbsp;screen, with clickable cross-references (<em>e.g.</em> from citations in the text to the list of References) and hyperlinks (<em>e.g.</em> for URLs and most References). A&nbsp;PDF viewer with &ldquo;back&ldquo; function is recommended.</p>",2016,"Integrative bioinformatics, Software integration, Data model, Ontology, Workbench, World Wide Web, Web service, BioXSD, EDAM, eSysbio, Free software, Open-source software, Accessibility, Reliability, Efficiency, Maintainability, Best practice, Community effort",10.5281/zenodo.33715,,publication
D1.2_Final Dissemination and Exploitation Plan,"Garavelli, Sara, Jones, Bob, Amsaghrou Rachida","<p>The following summary outlines the major assets and sustainability strategy that PICSE has produced during its eighteen month activity. This second iteration, which is the present report, combines the final strategy of the communication plan as well as addressing the exploitation of the project itself. The PICSE project, in its timeframe produced:</p>

<p><strong>Four PICSE Major Sustainable and Exploitable Assets </strong></p>

<ul>
	<li>
	<p><strong>PICSE Roadmap</strong><strong>, </strong>highlighting the existing challenges, barriers and trends in procuring cloud services and presenting a precise Call for Action for the three major stakeholders involved in the cloud procurement process: Public Research Sector Organisations, Cloud Service Providers and Policy Makers.</p>
	</li>
	<li>
	<p><strong>PICSE Cases Studies &amp; Best Practices</strong>, providing a set of lessons learnt and good practices at which public sector organisations can look at as a reference point when procuring cloud services.</p>
	</li>
	<li>
	<p><strong>The PICSE Wizard Tool</strong><strong> and a cloud procurement guide</strong><strong>, </strong>aimed to provide public research sector organisations new to the cloud with easy to use tools to facilitate and speed up the procurement of cloud services.</p>
	</li>
	<li>
	<p><strong>PICSE.eu platform</strong><strong>, </strong>a unique knowledge hub for public procurement of cloud services. Hosting all the main project assets, responds to several European policy priorities for cloud computing, by fostering trust in cloud services for the public sector.</p>
	</li>
</ul>

<p><strong>A successful learning experience setting the scene for a new joint Pre-Commercial Procurement</strong></p>

<ul>
	<li>
	<p><strong>PICSE&rsquo;s procurement model and networking activities</strong> <strong>has led to the formation of a joint Pre-Commercial Procurement (PCP) for innovative cloud services</strong> entitled HNSciCloud (2016-2018) Two of the PICSE partners are also involved. HNSciCloud aims to execute a joint tender led by 10 public research organisations across Europe, for which it will apply a revised and agile procurement model, based on the initial research performed by PICSE, and verify its applicability on a European scale. The experience gained will be used to refine the procurement model, which can establish a best practice for public sector procurements.</p>
	</li>
</ul>

<p>&nbsp;</p>

<p>&nbsp;</p>",2016,"Procurement, Cloud Services, Communication, Dissemination, Exploitation, Plan",10.5281/zenodo.49826,,publication
D2.2_Research Procurement Case Studies,Garavelli Sara,"<p>Digital transformation is absolutely crucial to any organisation whether public or private. It is at the very core of the digital single market, in order to ensure Europe reaps the socio-economic benefits of new technologies. Cloud computing has the potential to reduce IT expenditure and to boost organisational agility while at the same time expanding the scope for the delivery of flexible high-quality new services.<br />
Barriers to the adoption of cloud services range, shifting to new procurement processes matching the cloud&rsquo;s on-demand model, to lack of trust &amp; security, and finally from lack of mature technical standards to complex legal terms and fear of vendor lock-in. Overcoming these barriers is key to boosting public sector productivity and efficiency, and to meet the demands of a new set of users in a way that ensures secure and reliable and compliance with institutional requirements.<br />
This document describes a set of case studies documenting how public sector organisations worldwide have either carried out a process to procure cloud services, or are considering doing so. The experiences vary in term of success and offer insights into how the procurement of cloud services is impacting their current processes.<br />
Thirteen case studies are included; nine describe the experience of public sector organisations, which have procured different types of cloud services for different amounts of money; four others report the considerations of organisations that considering cloud procurement.<br />
The procurement experiences described in these case studies showcase that even if almost all of the organisations have the same procurement procedures, cloud procurement can be approached in many different ways. This is mainly due to different maturity levels with respect to cloud adoption in these organisations. However, as a common factor, almost all the organisations share the same challenges and agree on what would be the actions needed to quicken the cloud procurement process.<br />
Among the key lessons learned from the case studies are the following:<br />
&Oslash; Having the right skill set is fundamental for managing a successful procurement process;<br />
&Oslash; Legal aspects and Data Processor Agreements are fundamental for public entities to procure;<br />
&Oslash; Exit strategies when moving to the cloud should be carefully defined to avoid vendor lock-in;<br />
&Oslash; Writing an effective tender is of paramount importance: when writing tender specifications it is recommended to have some pre-discussions with potential providers to better understand the solutions they can offer;<br />
&Oslash; Standard and well-defined specifications work best;<br />
&Oslash; Marketing the tender is the first step to ensure that at least the minimum number of responses are received;<br />
&Oslash; Joint procurement actions can reduce the cost of developing specifications &amp; contracts and to improve purchasing conditions due to combined capacity;<br />
In addition, at least five wishes shared by all the procurers emerge from the case studies:<br />
1. More transparency in cataloguing services;</p>

<p>2. Catalogues of cloud service providers available for procurers&rsquo; consultation;<br />
3. A standalone test that could be used to verify the suitability of the services offered by the suppliers;<br />
4. Sample templates and guiding graphs to write public tenders;<br />
5. Accounting of cloud resources and comparing costs supported by standards.<br />
The outcomes of the case studies analysis is fundamental to the analysis of the cloud service procurement best practices performed in D3.2 and feeds the PICSE Roadmap on Cloud Service Procurement for public research organisations (D2.3).</p>",2016,"PICSE, Procurement, Case Studies, Cloud, Public Research Organizations, Best Practices ",10.5281/zenodo.46973,,publication
STUDY OF A SECURE AND PRIVACY-PRESERVING OPPORTUNISTIC COMPUTING FRAMEWORK FOR MOBILE-HEALTHCARE EMERGENCY,"Pramod B. Deshmukh, Nilesh N. Wani, Laxmikant S. Malphedwar, Deepali A. Ghanwat","<p><em>With the pervasiveness of smart phones and the advance of wireless body sensor networks (BSNs), mobile Healthcare (m-Healthcare), which extends the operation of Healthcare provider into a pervasive environment for better health monitoring, has attracted considerable interest recently. However, the flourish of m-Healthcare still faces many challenges including information security and privacy preservation. In this paper, we propose a secure and privacy-preserving opportunistic computing framework, called SPOC, for m-Healthcare emergency. With SPOC, smart phone resources including computing power and energy can be opportunistically gathered to process the computing-intensive personal health information (PHI) during m-Healthcare emergency with minimal privacy disclosure. In specific, to leverage the PHI privacy disclosure and the high reliability of PHI process and transmission in m-Healthcare emergency, we introduce an efficient user-centric privacy access control in SPOC framework, which is based on an attribute-based access control and a new privacy-preserving scalar product computation (PPSPC) technique, and allows a medical user to decide who can participate in the opportunistic computing to assist in processing his overwhelming PHI data. Detailed security analysis shows that the proposed SPOC framework can efficiently achieve user-centric privacy access control in m-Healthcare emergency. In addition, performance evaluations via extensive simulations demonstrate the SPOC’s effectiveness in term of providing high-reliable-PHI process and transmission while minimizing the privacy disclosure during m-Healthcare emergency.</em></p>",2016,"Security, Healthcare, Opportunistically, PHI & SPOC",10.5281/zenodo.192369,,publication
